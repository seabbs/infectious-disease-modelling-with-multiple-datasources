\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{authblk} % For author affiliations
\usepackage{hyperref} % For hyperlinks
\usepackage[margin=1in]{geometry} % Standard margins
\usepackage{natbib} % For bibliography management
\usepackage{enumitem} % For customising lists
\usepackage{color}
\usepackage{amsmath}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{array} 
\usepackage{pdflscape}
 \usepackage{booktabs}
\usepackage{tabularray}
\usepackage{soul}
\usepackage{tabularx}
\usepackage[printonlyused]{acronym}

\newcommand\Fontvi{\fontsize{8}{8}\selectfont}

\newlist{tree}{itemize}{10}
\setlist[tree]{label=-}
\setlistdepth{10} 
\usepackage{comment}

\title{A Workflow for Infectious Disease Modelling}

\author[1]{Sam Abbott}
\author[2]{Xiahui Li$^\dagger$}
\author[3]{Punya Alahakoon$^\dagger$}
\author[4]{Dhorasso Temfack$^\dagger$}
\author[5]{Johannes Bracher}
\author[6]{Felix GÃ¼nther}
\author[7]{Sabine Van Elsland}
\author[8]{Adrian Lison}
\author[-]{James Hay}
\author[10]{Oliver Eales}
\author[11]{Eben Kenah}
\author[10]{James McCaw}
\author[12]{Freya Shearer}
\author[1]{Sebastian Funk}
\author[13]{Mircea T. Sofonea}
\author[7, 14]{Pierre Nouvellet}
\author[15]{Daniela De Angelis}
\author[16]{Michael J. Plank$^*$}
\author[7]{Anne Cori$^*$}
\author[15]{Anne Presanis$^*$}

\affil[1]{London School of Hygiene \& Tropical Medicine}
\affil[2]{School of Mathematics and Statistics, University of St Andrews}
\affil[3]{University of Oxford}
\affil[4]{School of Computer Science and Statistics, Trinity College Dublin}
\affil[5]{Karlsruhe Institute of Technology}
\affil[6]{Robert Koch Institute}
\affil[7]{MRC Centre for Global Infectious Disease Analysis, School of Public Health, Imperial College London}
\affil[8]{Computational Evolution, ETH Zurich}
\affil[9]{Nuffield Department of Medicine, Univeristy of Oxford}
\affil[10]{School of Mathematics and Statistics, University of Melbourne}
\affil[11]{College of Public Health, Ohio State University}
\affil[12]{School of Population and Global Health, University of Melbourne}
\affil[13]{University of Montpellier}
\affil[14]{School of Life Sciences, University of Sussex}
\affil[15]{MRC Biostatistics Unit, University of Cambridge}
\affil[16]{School of Mathematics and Statistics, University of Canterbury}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{$^\dagger$Equal contribution}
\footnotetext[2]{$^*$Equal contribution}

\date{\today}

\begin{document}

\maketitle
\newpage
\begin{abstract}
%Lead: Sam Abbott
Infectious disease models can be used to inform critical public health decisions, yet often lack systematic development and validation practices.
The infectious disease modelling community has been slow to adopt rigorous Bayesian workflow approaches, even as these methods become increasingly formalised and widely used in other domains. Recent outbreaks have demonstrated some of the domain-specific challenges that infectious disease modelling faces, including evolving research questions, emerging data sources, and adapting surveillance systems.
Here, we suggest a workflow for developing infectious disease models, building on general Bayesian workflow advice, with a particular focus on these domain-specific challenges. This workflow is designed for anyone developing an infectious disease model, as well as for users of model outputs who need to be able to evaluate modelling studies. At each stage, we provide good practice recommendations based on our experience. We begin by outlining an approach for characterising epidemiological data source properties through a structured checklist. We then present an iterative workflow that extends established Bayesian model development approaches to the infectious disease domain, with the checklist informing decisions throughout each workflow stage. Our workflow includes defining the research question, development of directed acyclic graph (DAG) representations of process and observation models, model modularisation, inference and computation choices, model specification and validation, integration method selection, and real-world considerations. 
Throughout, we identify feedback loops where later decisions impact earlier choices. We provide an overview of approaches for each step as well as practical recommendations. In addition to outlining this workflow, we also give guidance on using it in evolving settings, such as outbreaks, and on good reporting practices. To demonstrate this workflow, we use schematic case studies of estimating transmission intensity with examples of navigating real-world trade-offs between model complexity, computational feasibility, and inferential goals.
These case studies highlight how different data types can provide complementary information but may also impact other workflow choices. 
Our suggested framework emphasises parsimony, interpretability, and model criticism. By establishing systematic workflow practices, this framework offers a foundation for improving both the quality and transparency of infectious disease modelling, particularly during rapidly evolving outbreaks where flexible, principled approaches are essential. 
\end{abstract}
\newpage

\tableofcontents
\newpage

\section{Introduction}
% Lead: Sam Abbott

% Paragraph 1: Motivation and Context
Infectious disease models can inform critical public health decisions, yet often lack systematic development and validation practices \citep{Ward2024-sp, Fyles2024-qz, Abbott2021-delta, Abbott2022-prevalence}.
COVID-19, mpox, and Ebola responses demonstrated both the value of infectious disease models and the limitations of rapid model development \citep{knock2021key, ro2025estimating, Abbott2021-delta, Abbott2022-prevalence, Ward2024-sp,birrell2025real}.
These outbreak settings create unique pressures where novel data streams emerge rapidly, surveillance systems evolve to meet changing needs, and models must be developed under severe time constraints with limited understanding of new data sources \citep{mccaw2023role}.
The infectious disease modelling community has not widely adopted rigorous Bayesian workflow practices that could address these challenges through iterative development, systematic criticism, and principled uncertainty quantification \citep{gelman2020bayesian, nicholson2022interoperability}.
This issue is particularly problematic when integrating multiple data sources, which is becoming increasingly common as it has been found to improve parameter estimation, reduce uncertainty, and provide more robust evidence for public health decision making \citep{deangelis2018analysing, sherratt2021exploring}.
However, integrating multiple data streams presents complex methodological choices, particularly when sources provide conflicting information.
While some recent work has partially implemented workflow elements, such as \citet{bouman2024bayesian}, these efforts lack structured guidance and omit key workflow components like prior predictive checks, systematic model criticism, and the integration of multiple data streams.
Existing guidance is also fragmented across the literature, with limited practical guidance for navigating these interconnected modelling choices systematically \citep{nicholson2022interoperability, deangelis2018analysing}.

% Paragraph 2: Current Approaches
Current approaches to infectious disease modelling broadly fall into two categories: pipeline methods that fit separate models to components sequentially passing the outputs from one model to the next as an input, and joint modelling approaches that simultaneously fit multiple data sources within a unified statistical framework \citep{deangelis2018analysing, russell2024combined}.
Pipeline approaches offer computational efficiency and modular development, but may propagate errors and fail to capture dependencies \citep{lison2024generative, Ward2024-sp}.
Joint modelling can provide more principled uncertainty quantification and better parameter identifiability, but often requires substantial computational resources and model complexity \citep{russell2024combined, lison2024generative}.
Recent empirical evidence supports the finding that joint approaches usually outperform stepwise methods for these reasons \citep{lison2024generative}.
Beyond these choices, there are numerous challenges when integrating multiple data sources, including: detecting and resolving conflicts between data sources, e.g. when different data streams suggest different dynamics; combining data sources with different spatial or temporal resolutions; and navigating branching decision paths where multi-data source integration choices impose model structure constraints \citep{deangelis2018analysing, nicholson2022interoperability}.
Fitting challenges, such as computational intractability, parameter non-identifiability, and the need to approximate ideal model structures for practical inference, further complicate implementation \citep{corbella2022inferring, Ward2024-sp}.
These integration and fitting considerations can impact model design, yet their implications are rarely made explicit in published analyses.

% Paragraph 3: Paper scope and contribution
This paper provides a systematic workflow for infectious disease modelling, along with recommendations based on our experience.
Our approach builds on established Bayesian workflows \citep{gelman2020bayesian}, extending them for the specific challenges of infectious disease modelling including evolving research questions, emerging data sources, and time-constrained development.
We advocate for Bayesian thinking throughout the model development process, including model specification, model criticism, and predictive checks, regardless of whether the final implementation uses Bayesian fitting methods.
We first outline an approach for characterising epidemiological data source properties through a structured checklist, then present an iterative workflow that extends established Bayesian model development approaches to the infectious disease domain, with the checklist informing decisions throughout each workflow stage.
Our workflow includes defining the research question, developing directed acyclic graph (DAG) representations of process and observation models, model modularisation, inference and computation choices, model specification and validation, integration method selection, and real-world considerations.
Throughout, we identify feedback loops where later decisions impact earlier choices.
We provide an overview of approaches for each step as well as practical guidelines for integration choices, validation strategies, and conflict resolution between data sources.
In addition to outlining this workflow, we also provide guidance for using it in evolving settings such as outbreaks and on good reporting practices.
Our workflow is designed for anyone who develops infectious disease models, whether in academic research or public health practice, as well as users of model outputs who need to evaluate modelling studies.
Finally, we use four case studies centred on reproduction number estimation to demonstrate this workflow, progressing in complexity from a single-source baseline, through integrating similar data sources, to more complex examples with data of different types and scales.
These case studies demonstrate broader principles applicable across infectious disease modelling contexts.
By focusing on this common estimation task, we provide a concrete foundation for adopting rigorous workflow practices that can then be extended to more complex modelling challenges.

\section{Data Sources and Characteristics} \label{sec:datareview}
% Lead: Punya Alahakoon

There is an increasing number of data sources being used for infectious disease surveillance and modelling, including case notifications, hospitalisations, viral load measurements from clinical samples, and wastewater samples. While each of these data sources offers unique advantages, such as timeliness, population coverage, and direct information on some quantities, they also come with distinct limitations, including reporting delays,  measurement noise, or lack of representativeness. There is currently no standardised approach to assess and compare these data sources in terms of their strengths and weaknesses, hindering our ability to compare, integrate, and prioritise data streams effectively. 

The WHO guidelines for integrated surveillance of respiratory pathogens with pandemic potential emphasise the importance of standardised, multi-purpose surveillance systems that also support public health responses and modelling \citep{world2024implementing}. Inspired by these, we propose six overarching characteristics with which to evaluate different data sources in a common framework (Figure \ref{data_characteristics}).  These characteristics are: (1) \textit{metadata}  that encapsulates foundational information about each data stream; (2) the \textit{scope} of a data source, i.e. its representative breadth and epidemiological relevance; (3) \textit{resolution} that describes the granularity and level of detail captured by a data source; (4) \textit{data quality} that encompasses the reliability, accuracy and completeness of the measurements within a data stream; (5) \textit{data utility} that refers to the practical applicability of a data source in informing epidemiological metrics and supporting public-health decision making; and (6)\textit{ practical considerations} that address the feasibility of using a data stream in modelling and surveillance contexts. We define more detailed potential criteria for each of these characteristics in the Supplementary Materials. 

We propose that these six categories are useful as a guide for the systematic assessment of a data stream's strengths and weaknesses. While the precise structure, relevance, and sub-attributes of each component may vary depending on the specific surveillance and modelling objectives, the key aim of these categories is to promote structured, transparent thinking. We suggest first assessing our checklist within your collaboration or team,  and then modifying it to better align with your own research question. Once happy with the checklist, review each dataset available in turn, ideally independently, and collate the responses. We then suggest summarising these responses, as demonstrated in our case study, and referring back to them through the remainder of the workflow.


\begin{figure}[H] 
\includegraphics[width=1\linewidth]{figures/data_characteristics_2.png}
\centering
\caption{The proposed checklist for characterising a data stream used in infectious disease surveillance and modelling. The six core characteristics---metadata, scope, resolution, data quality, data utility and practical considerations---are illustrated alongside representative attributes that can be used to assess the strengths and limitations of each data source. }
\label{data_characteristics}
\end{figure}


\section{Workflow}
\label{sec:workflow}
% Lead: Sam Abbott

The workflow should be implemented through multiple iterations, starting with high-level considerations and becoming more detailed with each pass, as some steps require decisions in later workflow components (e.g., posterior prediction requires inference and computation choices).

We start with clearly defining our \textbf{research question} and \textbf{target estimands} (Section~\ref{sec:research-question}, e.g. time-varying reproduction number, overdispersion parameters). Next, we suggest using \textbf{directed acyclic graphs (DAG)} to formalise dependencies between variables, thereby defining our model structure. However, we depart from standard approaches in that we suggest decoupling the definition of the \textbf{disease process} and \textbf{observation} models. The disease process model DAG (Section~\ref{sec:process}) represents the underlying epidemiological process, i.e. the transmission and related processes. The observation model DAG (Section~\ref{sec:observation}) links this process to measured data, accounting for measurement mechanisms and model misspecification. A key part of our workflow is iterating on this representation as understanding develops during an outbreak. 
Our next step is to use the data source characterisation from Section~\ref{sec:datareview} to \textbf{select available data sources} (Section~\ref{sec:data-selection}) that inform our disease process model, such as incidence time series, genomic data, contact tracing, viral load measurements, and serological surveys. Data sources should be selected based on their expected contributions to the research question. For each data source, we develop an observation DAG linking the underlying process to observed data through measurement models and reporting mechanisms. 

Once we have developed our disease process and observation DAGs, we \textbf{modularise} our joint model (Section~\ref{sec:modularise}) into separate sub-models for each data source and make \textbf{inference and computation choices} (Section~\ref{sec:fitting}): we select an inference method based on the model structure, and theoretical and practical considerations for each modular model.
We next decide how to \textbf{implement} our model in inferential and computational framework chosen (Section~\ref{sec:implementation}), ideally following software development best practices in a probabilistic programming language.
We apply \textbf{model specification and validation} (Section~\ref{sec:spec-validate}) to each module independently, including prior specification, parameter identifiability assessment, and prior and posterior predictive checks.
After validating individual modules, we then face key decisions on \textbf{data integration}, namely which modules to combine and how  (Section~\ref{sec:integration}).

If combining multiple modules is not beneficial or feasible, we can proceed to the combination of estimates, e.g. where multiple models are used to estimate the effective reproduction number from different data sources, and we combine the different estimates through ensemble approaches. If combining multiple modules is warranted, we select a data integration method and return to inference and computation choices (Section~\ref{sec:fitting}) for the newly combined model.

Throughout this workflow, several feedback loops are possible between steps, so that model development is rarely a single forward pass (see Figure~\ref{fig:workflow}). 
Data characteristics can alter process DAG structure: individual-level data requires different representations than population-level data. 
Inference and computation constraints may require approximating or restructuring the process DAG.
Identifiability issues discovered during model specification may require simplifying either the process model or observation models. 
Practical constraints shape integration choices in multiple ways. 
Teams using incompatible programming languages may necessitate non-joint approaches. 
Similarly, computational constraints may favour approximate over exact joint approaches or ensembling independent models.
Alternatively, they can suggest refining the underlying model DAGs.
Integration complexity itself may require simplifying observation models to maintain computational feasibility. 
Conflicts between data sources during integration often reveal model misspecification, requiring revision of earlier assumptions.

In the following sections, we discuss each of these steps in more detail and include references to relevant resources for further exploration.

\begin{landscape}
\begin{figure}[htbp]
    \centering
    \includegraphics[width = 1 \textwidth]{figures/restructured_workflow.drawio.png}
    \caption{\textbf{Recommended workflow for integrating multiple data sources in infectious disease modelling.} Key feedback loops from downstream parts of the workflow that impact earlier choices are represented with dashed arrows and boxes.}
    \label{fig:workflow}
\end{figure}
\end{landscape}

\subsection{Research Question and Target Estimands} \label{sec:research-question}

Clearly defining the research question and the epidemiological parameters to be estimated shapes the entire modelling workflow. Our definitions will result from the main modelling aims, e.g. forecasting hospital admissions or estimating the potential impact of interventions, and are best decided in collaboration with a range of stakeholders \citep{marshall2024when}.
Often, different public health objectives need knowledge of multiple quantities, that require balancing trade-offs. For example, we need both short-term forecasts for operational planning and effective reproduction number ($R_t$) estimates for public communication and interpretability. Our aim is to translate these policy needs into specific target estimands, by understanding how parameters inform decisions \citep{nicholson2022interoperability, gip-2024-ru}. Policymakers might need to know if transmission is increasing rather than precise values, or whether superspreading drives transmission, to choose between population-wide versus targeted interventions. We recommend defining the scope of an analysis explicitly, including what it cannot address: e.g. a national $R_t$ estimate cannot easily reveal local outbreak dynamics, aggregate case data cannot identify transmission chains, and symptom-based surveillance may not detect mild infections.
These limitations shape stakeholder expectations and prevent misuse of results.

Note that our research questions evolve throughout outbreaks, as understanding improves and policy needs shift. Early outbreak questions often focus on growth rates and severity using limited case data. As surveillance expands, questions might shift, for example, to understand variant dynamics, requiring genomic data integration; then to quantifying population immunity, needing serological data.

\subsection{Disease process DAG Development} \label{sec:process}

Directed acyclic graphs (DAGs) represent the joint distribution of all variables (both observed and latent) in a model, visualised as nodes. This joint distribution is factorised into the distribution of each variable conditional on its parent nodes, with directed edges representing these dependences. This formal representation makes conditional independence assumptions explicit and easily readable from the DAG, with the basic assumption being that any node is conditionally independent of its non-descendants given its parents \citep{lauritzen1996}. We recommend using disease process DAGs for structuring transmission models because they separate what drives epidemics (processes related to or affecting transmission between hosts) from how we observe them.  
This separation enables modular model development where process components can be reused across different surveillance contexts \citep{nicholson2022interoperability}.
Some common elements of disease process DAGs might be population structure, contact patterns, infection progression, and immunity dynamics \citep{deangelis2018analysing}. 
Our aim when constructing a disease process DAG is to translate our research question and target estimands into structures that can be evaluated, communicated, and refined.
Rather than starting from scratch, we recommend aiming to adapt established epidemiological models for similar pathogens or transmission routes, modifying components as outbreak-specific data emerges \citep{gelman2020bayesian}.
We can either begin model development simply and add complexity, or start comprehensive and simplify to essential components \citep{gelman2020bayesian}.
Generally, we recommend starting with a simple representation of core transmission dynamics, then iteratively refining as understanding improves or as needed by other parts of the workflow.
Early outbreak DAGs might represent homogeneous mixing, while later versions could incorporate age structure, spatial heterogeneity, or variant dynamics.
Key biological mechanisms shape disease process DAG structure. For example, incubation periods create latent compartments between susceptible and infectious compartments, with the numbers in each compartment represented by a node in the DAG; generation time distributions determine numbers of newly infected individuals with this dependence represented by a directed edge; and waning immunity adds nodes representing transition rates from recovered back to susceptible compartments.
 These structural choices directly affect transmission dynamics and parameter identifiability (Section~\ref{sec:spec-validate}). Many epidemic processes can be described using state-space models, where nodes in the disease process DAG represent latent states that evolve over time according to stochastic or deterministic rules \citep{birrell2018evidence}. These states capture unobserved infection dynamics and their temporal progression, providing a framework for linking the disease process to observed data (Section~\ref{sec:observation}) and for systematically expanding the model as new biological mechanisms or outbreak features are incorporated.

\subsection{Data Source Selection} \label{sec:data-selection}

With the disease process DAG and target estimands defined, we next select appropriate data sources, drawing from the data review (Section~\ref{sec:datareview}), based on their characteristics and the specific epidemiological quantities they inform.
For example, case data can be a proxy for infection incidence with reporting delays and ascertainment bias, hospitalisations capture severe outcomes, and wastewater indicates population-level pathogen shedding, independent of healthcare-seeking behaviour.
In principle, integrating multiple data sources should enhance the identifiability and precision of parameter estimates \citep{deangelis2018analysing, lison2024generative, russell2024combined, birrell2025real}. In practice, however, data integration may pose challenges. 
These include inconsistencies due to unaccounted biases \citep{presanis2013conflict,knock2021key, Ward2024-sp, corbella2022inferring}; computational complexity \citep{corbella2022inferring}; and operational constraints during an outbreak emergency \citep{mccaw2023role}.
We therefore recommend starting with minimum data requirements for target estimands and then expanding systematically. 
We recommend evaluating complementarity among data sources, by assessing whether they provide independent signals on different parameters. For example, wastewater surveillance may provide early warning of outbreaks, while clinical surveillance provides individual-level severity data. Multiple data sources may alternatively inform the same parameter, reinforcing evidence on that parameter and introducing some redundancy. While this redundancy can enhance precision, consistent with meta-analytic principles \citep{deangelis2018analysing,borenstein2021introduction}, it can also increase model complexity.

We recommend prioritising data sources based on their expected information gain relative to implementation cost and feasibility. ``Value of information'' methods \citep{jackson2019value,heath2024value} can formally guide whether additional data would meaningfully improve precision. However, for complex models especially, identifiability may only become evident after fitting an initial version.

\subsection{Observation DAG Construction} \label{sec:observation}

Observation DAGs map how latent epidemiological processes generate observed data, with complexity shaped by target estimands and available data sources \citep{deangelis2018analysing}.
We propose building these DAGs by thinking through each intermediate step that leads from the disease process to observation.
For example, for clinical surveillance data, steps may include symptom-based healthcare seeking, obtaining a sample to be tested, testing in a laboratory, and reporting; whilst for wastewater surveillance they may describe pathogen shedding, sewage transport, lab processing, and quantification.
Each step can introduce delays, biases, or missing data mechanisms that affect inference. Our observation model therefore should explicitly account for them. For example, we need to adjust for reporting and other delays that create temporal misalignment between transmission events, disease sequelae and observations  \citep{seaman2022nowcasting}. It is also important to distinguish between different missing data mechanisms since random missingness and systematic under-ascertainment require different handling \citep{sherratt2021exploring}. We should also account for time-varying observation probabilities, e.g. driven by changes in testing capacity, healthcare seeking or surveillance intensity changes. Observation components should ideally be modular and replaceable  \citep{gelman2020bayesian}. For example, initial Poisson noise models can become negative binomial to account for overdispersion. This flexibility reduces initial specification burden whilst enabling systematic refinement.
As for the disease process DAG, we recommend starting with a simplified observation DAG and iteratively increasing complexity as required by later stages of the workflow and the research question.
Estimating population-level transmission requires simpler observation models than understanding variant-specific dynamics or age-stratified patterns.
Individual-level data enables different observation models compared to aggregated reports.
An important challenge when constructing the observation process/model using multiple data sources is how we handle dependencies between them. For example, data streams may relate to the same underlying epidemiological process; or surveillance systems may have dependencies i.e., individuals may be captured by two or more systems. Developing a DAG will help us identify such dependencies.
Finally, we must account for hierarchical structure and population heterogeneity when demographic subgroups are present in the data or are central to the research question. We can validate our understanding of observation processes when we have multiple observation DAGs for the same underlying process by checking whether different data streams imply consistent transmission dynamics when properly integrated.

\subsection{Refining the model DAGs} \label{sec:refine-dags}

After mapping data sources and constructing our observations DAGs, we revisit our disease process and observation DAGs to ensure alignment between what we want to model and what our data can support.
This step of the workflow aims to create a single joint DAG composed of our process and observations DAGs.
Data availability can drive both increases and decreases in model complexity.
Contact tracing data with identified transmission pairs allows us to shift from population-level compartmental models to individual-based representations that capture heterogeneous mixing patterns.
Conversely, discovering that surveillance only provides aggregate counts may require collapsing a planned age-structured model into simpler compartments.
Strong prior information can sometimes compensate for data limitations, providing a further source of evidence.
For example, well-informed priors about transmission parameters from previous outbreaks might support maintaining a complex transmission model even when we have limited data; and detailed knowledge about age-specific contact patterns from previous studies could support using age structure, even if we only have total case counts.
Note that assuming fixed parameter values, common in mechanistic modelling literature, is effectively placing infinitely strong priors: we advocate avoiding such fixed values where possible in favour of appropriately uncertain prior distributions.
Temporal resolution affects what variation can be identified rather than what can be modelled: we can always build a daily-scale model, but weekly surveillance data will usually inform weekly or longer-term patterns, not daily fluctuations.
Additional DAG iteration typically occurs at key workflow stages (Figure~\ref{fig:workflow}), such as during model specification when identifiability issues emerge, during validation and/or data integration, when misalignment between model structure and data patterns is exposed, or due to practical or theoretical considerations related to model fitting \citep{corbella2022inferring}.
Each iteration should address specific problems rather than making arbitrary changes.

We may be unable to use domain knowledge or reasoning to produce a single valid DAG and instead have several candidates for either or both the disease process and observation DAGs. For example, independent teams working on a similar problem might generate different DAGs, particularly when mechanisms are not well-known or the data-generating process is poorly reported. If so, we proceed through the remainder of the workflow with each candidate DAG, comparing them in \ref{sec:spec-validate} and potentially integrating them in \ref{sec:integration}.

\subsection{Modularising DAGs} \label{sec:modularise}

After selecting data sources and developing and iterating on our DAGs, we decompose our proposed model into modular sub-models.
Each sub-model should be as simple as possible, ideally including the disease process DAG and a single observation DAG, therefore depending on a single data source.
We may need to make additional simplifying assumptions, which will be relaxed during the data integration step.

We start with simple sub-models as it is easier to diagnose issues such as poor fit, model misspecification or convergence issues for modules than for complex models.
We add complexity incrementally, only as far as needed.
This modularisation facilitates detection of inconsistencies or conflicts between data sources \citep{presanis2013conflict,manderson2023combining} and offers computational advantages over joint modelling \citep{deangelis2018analysing,goudie2019joining,gelman2020bayesian,nicholson2022interoperability}.

We apply the following workflow stages (Inference and Computation Choices, Implementation Considerations, Model Specification and Validation) to each module independently, then, if validated, we proceed to Data Integration Choices (Section \ref{sec:integration}) to determine how to combine them.
The integrated model may then require additional cycles through the workflow.

\subsection{Inference and Computation Choices}\label{sec:fitting}
% Lead: Xiahui and Dhorasso with support from Sam Abbott

Inference for infectious disease models involves estimating parameters $\boldsymbol{\theta}$ from observed data $\boldsymbol{Y}$.
We assume these data arise from a probabilistic model with a likelihood function $P(\boldsymbol{Y} |
  \boldsymbol{\theta})$ that links parameters to data as specified in our disease process and observation DAGs (Sections \ref{sec:process} and \ref{sec:observation}).
Bayesian inference, our recommended approach, updates prior beliefs $P(\boldsymbol{\theta})$ with the likelihood to obtain posterior distributions $P(\boldsymbol{\theta} | \boldsymbol{Y}) \propto P(\boldsymbol{Y}|\boldsymbol{\theta}) P(\boldsymbol{\theta})$, providing principled uncertainty quantification.
Inference requires balancing model complexity, computational feasibility, likelihood tractability, and available expertise.
We structure inference and computation decisions through four stages: model complexity, selecting inference methods, implementation, and diagnostics (Figure \ref{fig:fitting}). 
These choices create feedback loops within the workflow where computational constraints may require approximating or restructuring earlier DAG specifications.
This section provides an overview of these considerations.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/restructured_fitting_workflow.drawio.pdf}
    \caption{\textbf{Inference and computation choices workflow for integrating multiple data sources in infectious disease modelling.} The workflow contains four interconnected components: Model Complexity Assessment (7.1) evaluates dimensionality, hierarchical structure, likelihood tractability, and computational constraints; Inference Method Selection (7.2) chooses between likelihood-free approaches and likelihood-based methods; Implementation Considerations (7.3) address software choices and compatibility requirements; and Diagnostics (7.4) involve convergence checks, sensitivity analysis, and computational trade-offs. The interconnected arrows indicate that these components inform each other iteratively, allowing researchers to optimize the balance between model complexity, computational feasibility, and inferential accuracy when integrating multiple data sources. The workflow includes key feedback mechanisms: inference and computation constraints may require approximating or restructuring the process DAG, leading back to stage 2 Process DAG Development. }
    \label{fig:fitting}
\end{figure}

\subsubsection{Model Complexity Assessment}

Before determining the inference approach, it is important we assess the complexity of the model.
Parameter dimensionality and structure are key to method selection as low-dimensional problems with fewer than 10 parameters require different approaches than problems with hundreds or thousands of parameters (see ``Handles high-dim. params.?'' in Table \ref{tab:methods_comparison}).
Similarly, strongly correlated parameters, discrete parameter spaces, or mixed continuous-discrete problems each impact which methods we can use or how they are implemented.
We also need to consider the number of latent variables, such as unobserved infection times, individual-level disease states, or spatially varying transmission rates.
When there are many latent variables relative to observed data, computational feasibility becomes a constraint that rules out certain approaches.
The tractability of the likelihood determines which methods can be applied.
Key questions include whether we can write the likelihood $P(\boldsymbol{Y}|\boldsymbol{\theta})$ analytically, evaluate it numerically at reasonable cost, or differentiate it with respect to parameters.
Models with stochastic differential equations or discrete event processes often lack tractable likelihoods, ruling out methods that require likelihood evaluation or gradients.

\subsubsection{Inference Method Selection}

Numerous inference methods and algorithmic variants exist within each broad class of approach, with distinct computational characteristics and practical trade-offs.
Rather than adopting a universal approach, our choice should be tailored to the specific modelling context and inferential goals.
The following gives guidance to selecting a method, but does not cover all settings.

Gradient-based Markov Chain Monte Carlo (MCMC) approaches represent our preferred methods when applicable due to their combination of efficiency, reliability, and ease of use \citep{gilks1995markov, lekone2006statistical}.
MCMC methods generate samples from the posterior distribution by constructing a Markov chain that converges to the target distribution, providing a flexible framework for Bayesian inference when analytical solutions are unavailable.
Hamiltonian Monte Carlo (HMC) leverages gradient information for efficient MCMC exploration of differentiable likelihoods, with its adaptive variant No-U-Turn Sampler (NUTS) representing our default recommendation for models with continuous parameters, or where discrete parameters can be marginalised out \citep{duane1987hybrid, hoffman2014no, andrade2020evaluation}.
When implementing gradient-based methods, the automatic differentiation method can have a substantial impact: forward-mode differentiation suits models with few parameters (typically fewer than 10), whilst reverse-mode differentiation scales better for high-dimensional parameter spaces.
It is important to test different automatic differentiation options when possible.
When full uncertainty quantification is less critical, Variational Inference (VI) approximates the posterior by optimising tractable surrogate distributions, offering fast approximation with modern variants like Pathfinder \citep{blei2017variational, chatzilena2019contemporary}.

Sampling-based approaches become necessary when gradients are unavailable.
Standard MCMC methods including Metropolis-Hastings and Gibbs sampling handle discrete or mixed parameter spaces \citep{hastings1970monte, geman1984stochastic, gilks1995markov}.
For discrete parameters, we should consider whether marginalisation is possible before resorting to sampling, as analytical integration can significantly improve efficiency and mixing.
Parallel tempering extends these methods to multimodal posteriors by running multiple chains at different temperatures, with modern implementations available in packages like Pigeons.jl \citep{surjanovic2023pigeons}.
For models in which the latent state dimension grows over time, as in state-space models, MCMC becomes increasingly inefficient.

Sequential Monte Carlo (SMC) methods, or particle filters, offer a natural alternative in models with latent states, such as Partially Observed Markov Process (POMP) models \citep{king2016statistical}. These methods approximate the posterior distribution of latent states over time using many parallel simulations (referred to as particles), each weighted according to how well it matches the observed data. This enables online Bayesian updating as new data arrive, making them well-suited for real-time epidemic tracking \citep{doucet2001introduction, birrell2020efficient, storvik2023sequential}. A key property of SMC is that it provides an unbiased estimate of the likelihood, which makes it particularly useful when embedded in parameter inference schemes. Particle MCMC (PMCMC) exploits this by combining SMC with MCMC for joint stateâparameter inference \citep{andrieu2010particle, endo2019introduction}, while SMC$^2$ extends the idea to fully sequential Bayesian updating as observations accumulate \citep{chopin2013smc2, TEMFACK2025100847}. Despite these strengths, particle-based methods face challenges such as particle degeneracy (where few particles carry most weight) and memory demands with long time series. Table \ref{tab:methods_comparison} summarizes their main advantages and limitations.
 
Approximate Bayesian inference methods provide efficient alternatives for specific model structures.
Integrated Nested Laplace Approximation (INLA) offers a computationally efficient method for approximating posterior marginals in latent Gaussian models, making it particularly effective for spatiotemporal disease models with spatial correlation structures \citep{rue2017bayesian}.
This method excels when models can be reformulated with latent Gaussian components but struggles with non-Gaussian latent structures.

Deterministic approaches focus on point estimation rather than uncertainty quantification.
Maximum Likelihood Estimation (MLE) provides point estimates for model parameters, particularly useful for simple models requiring only parameter estimates rather than full uncertainty quantification, or for prototyping \citep{myung2003tutorial, baltazar2024maximum}.
Profile likelihood methods extend MLE by examining the likelihood surface for individual parameters whilst optimising over others, providing confidence intervals without full posterior sampling \citep{tonsing2018profile, plank2024structured}.

Likelihood-free approaches are methods of last resort due to their computational demands and limited diagnostic capabilities.
These simulation-based methods bypass likelihood evaluation entirely, instead using model simulations to approximate the posterior distribution.
Approximate Bayesian Computation (ABC) approximates the posterior distribution by accepting parameter values that generate simulated data close to the observed data under a predefined distance metric, whilst synthetic likelihood methods approximate the likelihood by assuming Gaussian distributions for summary statistics \citep{beaumont2002approximate, wood2010statistical, price2018bayesian}.

\begin{landscape}
\begin{table}[ht]
\renewcommand{\arraystretch}{1.2}
\centering
\caption{\textbf{Comparison of Likelihood-Based and Likelihood-Free Fitting Methods.} 
This table focuses on foundational algorithms. More recent methodological advancements that build upon these classical approaches are discussed in Section~\ref{sec:fitting}. ``PPL'' stands for Probabilistic Programming Languages. For this review, PPL examples include Stan, PyMC, JAGS, NIMBLE, Turing.jl, etc.}
\label{tab:methods_comparison}
\small
\begin{tabular}{@{}p{3.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}@{}}
\toprule
\multirow{2}{*}{\textbf{Feature}} & \multicolumn{6}{c}{\textbf{Likelihood-Based}} & \multicolumn{2}{c}{\textbf{Likelihood-Free}} \\
\cmidrule(lr){2-7} \cmidrule(lr){8-9}
 & MLE & MCMC & SMC & PMCMC & INLA & VI & ABC & BSL \\
\midrule
\textbf{Theoretical Considerations} & & & & & & & & \\
\midrule
Requires likelihood? & Yes & Yes & Yes & Yes & Yes & Yes & No & No \\
Handles high-dim. params.? & Poor & Moderate & Moderate & Moderate & Good & Good & Moderate & Moderate \\
Convergence guarantees & Asymptotic & Asymptotic & Asymptotic & Asymptotic & Approx. & Approx. & Approx. & Approx. \\
Distributional flexibility & Low & High & High & High & Medium & Medium & High & Medium \\
Approximation error & Exact (asymp.) & Exact (asymp.) & Particle-based & Exact (asymp.) & Deterministic & Variational & Simulation & Simulation \\
\midrule
\textbf{Practical Considerations} & & & & & & & & \\
\midrule
Computational cost & Low & High & Med--High & Very high & Low & Low--Med & High & High \\
Scalability (big data) & Good & Poor & Moderate & Poor & Good & Good & Poor & Moderate \\
Example software & PPL & PPL & LibBi & POMP, NIMBLE & R-INLA & PPL & abctools, EasyABC, ELFI & ELFI \\
Tuning required? & Minimal & Step size, priors & Resampling & Complex & Minimal & ELBO opt. & Sum. stats., distance, threshold & Sum. stats. \\
Capable of real-time inference? & Yes & No & Yes & No & Yes & Yes & No & No \\
Parallelization potential & 
High & 
Limited, chain-level, GPU possible but hard & 
High, particle-level, GPU possible & 
Limited, particle-level, GPU possible for simple models & 
Low, matrix operation hard to parallelise & 
Medium, gradient parallelisation; GPU possible & 
High, simulations parallelisable; GPU possible & 
Medium, GPU possible \\
Diagnostics & Likelihood ratio & $\hat{R}$, ESS, Trace& ESS & $\hat{R}$, ESS,Trace & Residuals & ELBO, PPC & Acc.~rate, PPC & Acc.~rate, PPC \\
Best use case & Simple models & General Bayesian inference & Real-time inference & State-space models & Latent Gaussian models & Fast approximation & Intractable likelihood & Intractable likelihood + summary stats \\
\bottomrule
\end{tabular}
\end{table}
\end{landscape}

% Table \ref{tab:methods_comparison} provides detailed comparison across multiple criteria to support these decisions.

\paragraph{Hybrid and Nested Approaches}

Modern probabilistic programming languages enable sophisticated combinations of inference methods.
INLA can be nested within NUTS, using efficient spatial approximations while sampling other parameters.
Languages like Turing.jl support mixed samplers, applying HMC to continuous parameters whilst using Gibbs for discrete components \citet{fjelde2025turing}.
SMC methods can incorporate HMC kernels for parameter moves, leveraging particle filtering for states and gradient-based sampling for parameters \citet{buchholz2021adaptive, devlin2024no, rosato2024enhanced}.
These hybrid approaches capitalise on the strengths of multiple methods.

When complex methods fail to initialise properly within prior bounds, simpler methods can provide starting values for more sophisticated approaches.
VI often proves effective for initialisation due to its speed and robustness, whilst MLE can provide reasonable point estimates as initial values.
However, initialisation difficulties may indicate unrealistic prior specifications, suggesting model validation should be revisited before proceeding with complex inference.

\subsubsection{Implementation considerations}

We recommend balancing computational cost against inferential quality when implementing a chosen inference method, considering both model complexity and available computational resources (Table \ref{tab:methods_comparison}) \citep{funk2020choices}.
We also recommend prioritising methods with automatic tuning to reduce implementation burden and improve reliability.
HMC and NUTS require minimal manual intervention through automatic step size adaptation and mass matrix estimation, making them relatively straightforward to use.
Standard MCMC and particle methods demand careful tuning of proposals, particle counts, and resampling thresholds through pilot runs.
ABC approaches require the most expertise for selecting summary statistics, distance metrics, and tolerance thresholds.
INLA and VI may require upfront model reformulation to fit their frameworks.

Parallelisation can determine whether methods can scale to population-level inference or handle real-time analysis during outbreaks.
Methods differ substantially in their parallelisation capabilities (Table \ref{tab:methods_comparison}).
MCMC methods face inherent parallelisation challenges due to their sequential chain structure.
Multi-chain parallelisation provides the primary scaling mechanism.
Parallel tempering exploits multi-core architectures through temperature-parallel chains \citep{surjanovic2023pigeons}.
Some implementations support within-chain parallelisation for gradient calculations (Stan and Turing.jl) or matrix operations (GPU acceleration).
VI scales well through mini-batch parallelisation and routinely uses GPU acceleration for large datasets \citep{hoffman2013stochastic, Abbott2021-delta}.
Sequential and particle methods excel at parallelisation through particle-level parallelism.
Modern GPUs enable thousands of particles to be processed simultaneously.
Memory requirements scale with particle count \citep{henriksen2012parallel}.
Approximate methods show mixed potential for parallelisation.
INLA has limited opportunities due to sequential matrix operations, though some implementations exploit parallel linear algebra libraries.
Nested approximations can parallelise across spatial or temporal components when model structure permits.
Likelihood-free methods provide parallelisation opportunities for rejection-based ABC.
Rejection ABC parallelises trivially across CPU clusters and GPU architectures.
ABC-MCMC retains the sequential constraints of MCMC methods.
Memory requirements can become substantial for large simulated datasets \citep{kulkarni2022hardware}.
Deterministic approaches parallelise most effectively.
MLE is highly parallelisable across data points on both CPUs and GPUs.
Profile likelihood calculations naturally parallelise across parameter values.

\subsubsection{Diagnostics}

The quality and availability of diagnostic tools vary substantially across inference methods, representing a key factor in method selection (see Table \ref{tab:methods_comparison} for method-specific diagnostics).
Implementations of HMC and NUTS provide the gold standard with divergence warnings, \ac{BFMI}, energy plots, and traditional MCMC diagnostics like $\hat{R}$ and \ac{ESS}, making them easier to use reliably.
Standard MCMC and parallel tempering offer moderate diagnostic capabilities through traditional convergence statistics and trace plots, with parallel tempering adding temperature swap rates to monitor multimodal exploration.
SMC and PMCMC methods rely on particle-specific metrics, including \ac{ESS} and weight degeneracy to track sequential performance and detect particle impoverishment. 
In contrast, VI provides only \ac{ELBO} convergence monitoring (which does not guarantee accurate posterior approximation), whilst ABC methods offer minimal diagnostics through acceptance rates alone.
Diagnostic failures across any method indicate potential model specification issues requiring validation (Section \ref{sec:spec-validate}), with specific issues like divergent transitions in NUTS suggesting specific problems, like posterior geometry problems in this case.

\subsection{Software Implementation}\label{sec:implementation}

For implementing our models, we recommend using a probability programming language (PPL).
PPLs and their ecosystems make many tasks in model specification and validation more straightforward (Section \ref{sec:spec-validate}).
The precise choice often depends on the inference approach selected in the previous section (Section \ref{sec:fitting}), and in some cases, using a PPL may not be possible.
Generally, using the same implementation approach for all model components will require the least amount of effort.

Generative PPLs where we can sample from and compute gradients for the same model specification are particularly valuable.
Turing.jl, PyMC, and NumPyro provide this capability \citep{ge2018turing,fjelde2025turing,abril2023pymc,phan2019composable}, enabling simulation-based validation approaches without separate data generators.
Stan offers the most mature diagnostic and validation tools \citep{carpenter2017stan} , but requires separate simulation code.
JAGS and NIMBLE offer flexibility for model structures that may not be well-suited to gradient-based methods \citep{plummer2003jags,valpine2017programming}.
Turing.jl is likely the most composable option \citep{ge2018turing}, which proves useful when building models from modules.
Speed is difficult to assess and is often model and context-specific, though generally Stan and NumPyro are the most efficient options when applicable \citep{carpenter2017stan,phan2019composable}.
Selecting a flexible implementation approach is important for evolving settings (Section \ref{sec:outbreak}).

Making use of standard software development practices is likely to improve all aspects of our workflow \citep{gelman2020bayesian}.
These include: unit testing to verify model components, continuous integration to catch errors early, version control to track model evolution, and comprehensive test coverage for data processing and diagnostics.

%\subsubsection{Building Modular DAG Components}

Once we have chosen our software implementation, we translate our modularised DAGs (Section \ref{sec:modularise}) into separate code modules.
We structure observation models as interchangeable components connecting to disease process models through defined interfaces (e.g., both case and wastewater modules interface through infection incidence). We recommend: documenting interfaces explicitly to enable component reuse across teams; maintaining consistent parameter naming between mathematical formulation and code when possible; and starting with minimal implementations, adding complexity only as validation confirms necessity.

\subsection{Model Specification and Validation}\label{sec:spec-validate}

We advocate systematic Bayesian workflow for model validation \citep{gelman2020bayesian}, working through structured phases that provide clear decision points and feedback loops.
We start by validating models in small subcomponents first, then explore combinations to identify integration challenges early.

\subsubsection{Prior Specification and Prior Predictive Checks}

We specify priors using domain knowledge where available through informative priors; otherwise, we use weakly informative priors that regularise without dominating the likelihood.
Prior predictive checks simulate data from the prior to ensure model behaviour is epidemiologically plausible before seeing data and to detect prior-data conflict \citep{Box1980,yang2025detecting}.
These checks can reveal several issues requiring model modification: unrealistic parameter ranges may indicate the need for different priors or model constraints; implausible epidemic dynamics may suggest DAG structural changes; computational issues during simulation may indicate need for model approximations or reparameterisation.
Results of prior predictive checks may indicate a need to either: revisit prior specification, e.g. to obtain additional data sources, previous estimates or expert consultation to inform appropriate prior distributions; or return to DAG development (Sections \ref{sec:process} and \ref{sec:observation}) to simplify models, add model components, increase complexity through hierarchical structure, or implement approximations for computational tractability.

\subsubsection{Model Fitting and Computational Validation}

After resolving issues raised by prior-predictive checks, we fit the model using our chosen method from Section \ref{sec:fitting}. We validate computational aspects, such as convergence and algorithm performance, through diagnostic checks, synthetic data simulation, and simulation-based calibration where feasible.
Using simulated scenarios to validate parameter recovery provides valuable insights into model performance \citep{bouman2024bayesian}, whilst simulation-based calibration for each submodel provides stronger validation by verifying that inference algorithms can recover true parameters from simulated data \citep{talts2018validating}.
We suggest addressing computational issues systematically: simplify model structure or increase prior informativeness if convergence fails; examine submodels in detail if joint fitting proves intractable; reparameterise models if mixing is poor; plot intermediate quantities like effective reproduction numbers to identify problematic components; run on data subsets or reduce iterations to speed up iteration; and check for multimodality using multiple chains or parallel tempering \citep{gelman2020bayesian}.
Our goal is to make predictions or learn about epidemiological processes, not merely achieve convergence of an arbitrary model. 
If computational validation succeeds, we provisionally accept the model for posterior validation.
Persistent computational issues require returning to model specification or considering alternative integration approaches (Section \ref{sec:integration}).

\subsubsection{Posterior Validation}

We use posterior- and mixed-predictive checks \citep{rubin1984bayesianly,gelman1995bayesian} to assess whether fitted models reproduce key features across all integrated sources.
These checks form the foundation of model validation by comparing model-generated predictions against observed patterns.
To avoid overconfidence due to double use of the data and to respect temporal data structures, we should apply cross-validation strategies, particularly leave-one-out and time-aware validation.
The influence of individual data points and prior specifications should be examined through sensitivity analysis.

To assess parameter identifiability, we examine whether data sources provide sufficient information to uniquely estimate target parameters: joint modelling often resolves identifiability issues that plague pipeline approaches \citep{lison2024generative, russell2024combined}. Value of information approaches \citep{jackson2019value,heath2024value} can indicate where additional data or prior information is needed.  

Conflict in the context of data integration refers to when sources of information provide inconsistent or incompatible evidence on model parameters. Such conflicts may arise between prior assumptions and data (``prior-data conflict''), between different data sources, or even within a data source but between different units of data \citep{presanis2013conflict,yang2025detecting}.
They typically manifest as lack of identifiability (with algorithm convergence problems as a symptom) or a lack of fit to one or more data sources \citep{presanis2013conflict,deangelis2018analysing}. Detection methods based on generalising cross-validatory posterior-/mixed- or prior-predictive checks to any latent node in a DAG \citep{presanis2013conflict,yang2025detecting} usually also measure the extent of conflict. We can use complementary prior sensitivity analysis to quantify how much priors must change to accommodate observed data, revealing potential model misspecification or data quality issues \citep{Roos2015,Kallioinen2024,yang2025detecting}. We resolve conflicts through careful consideration of unaccounted observational biases or overly restrictive model assumptions. We adjust model structure as needed, including loosening prior assumptions, implementing weighting approaches to account for selection biases, and other bias adjustment approaches \citep{deangelis2018analysing}.

If posterior validation reveals model inadequacies, we return to DAG development (Sections~\ref{sec:process} and \ref{sec:observation}) to modify model structure, add complexity, or implement different approximations.

\subsubsection{Model Comparison}

When multiple plausible model specifications exist, we recommend using Leave-One-Out Cross-Validation (LOO-CV), Watanabe-Akaike Information Criterion (WAIC), out of sample Continuous Ranked Probability Score (CRPS), or other methods to compare predictive accuracy \citep{vehtari2017practical,yao2018using,gneiting2007strictly} combined with visual and/or quantitative posterior-predictive checks.
High Pareto-k values from LOO-CV identify influential observations requiring careful modelling.
For infectious disease models, we consider scoring transformations such as the log when using CRPS \citep{bosse2023scoring}.
We aim to maximise sharpness subject to calibration: models should produce statistically consistent predictions whilst minimising required uncertainty \citep{gneiting2007strictly}.
Model comparison serves as the final validation step, ensuring that our chosen specification provides optimal predictive performance whilst maintaining epidemiological interpretability.

\subsection{Data Integration Choices}\label{sec:integration}
% Lead: Anne Presanis

Decisions about data integration are inherently context-dependent, but can be guided by the decision tree in Figure \ref{fig:integration}. We recommend combining the modularised models sequentially and repeating the validation processes outlined in \ref{sec:spec-validate} for each newly combined module, repeating this process until the full model has been implemented and validated.
We further recommend using joint modelling of these modules where possible, taking advantage of Markov melding \citep{goudie2019joining} or its approximations where possible, to achieve the joint model in a computationally efficient way (Section \ref{sec:joint}). We can also make use of cut distributions \citep{plummer2015cuts} to downweight the influence of less trusted modules if needed. 
This joint modelling approach offers a coherent framework for integrating such sources simultaneously, in the spirit of Bayesian hierarchical models \citep{gelman2020bayesian,deangelis2018analysing}, with suitable uncertainty quantification and avoiding the definitional ambiguities that affect output-level combinations \citep{manley2024combining, brockhaus2023why}. 

Ensemble methods (Section \ref{sec:ensembling}) are valuable when joint modelling proves impossible due to computational constraints, data access limitations, or institutional boundaries between research teams.
When employing ensembles, careful attention to ensuring all models target the same estimand, resolving temporal alignment, and documenting methodological assumptions becomes essential for meaningful combination. The choice of ensembling or meta-analytic approach should align with the research objective (e.g. estimation or prediction), influencing how estimates or predictions are weighted (e.g. by precision or prediction accuracy). In contrast, when integrating sub-models, the approach will depend on whether existing sub-models have already been fitted or if the models are being developed de novo.

In addition to prior-data conflict and between-data source conflict, different modules may also conflict. These types of conflict should be assessed during the model validation and specification process, as discussed in Section \ref{sec:spec-validate} \citep{presanis2013conflict,sherratt2021exploring,yang2025detecting}, once the data integration process has been decided upon.

\subsubsection{Full joint modelling}\label{sec:joint}

A fully joint modelling approach is especially suited when there are complex dependencies between data sources \citep{corbella2022inferring}, perhaps conditional on the parameters of the underlying disease process model, such that identification of all parameters is compromised when some data are left out. However, joint models can be computationally intensive and challenging to fit, especially in high-dimensional or real-time settings. Care is needed to ensure complex dependencies are appropriately modelled and that observational biases, such as selection biases, are adequately accounted for in the model, to avoid conflict between data sources \citep{presanis2013conflict,corbella2022inferring}. 

Markov melding \citep{goudie2019joining} allows a full joint model to be fitted in a computationally efficient approach, by combining sub-models in such a way that the melded posterior is the full joint posterior. This posterior is achieved by using posterior samples from one sub-model to serve as a proposal distribution for the next, potentially in a sequential chain of models \citep{manderson2023combining}. Similar sequential strategies, using the posterior from one sub-model as the prior (rather than proposal) for another, have a long history \citep{west1997bayesian}, closely related to Sequential Monte Carlo algorithms \citep{doucet2001introduction} (see Section \ref{sec:fitting}).

While the theory and some applications of Markov melding are well-established \citep{goudie2019joining,nicholson2022interoperability,manderson2023combining}, practical implementation remains challenging. Limitations include the availability of user-friendly Bayesian software and methods for assessing sub-model compatibility and combining them \citep{yang2025detecting}. As a result, approximate methods are often used. A common approach is the one used in standard meta-analysis \citep{borenstein2021introduction} to combine previous estimates using a normal approximation. A Stage 1 point estimate, $\hat{y}_1$, and its corresponding standard error or posterior standard deviation $\hat{\sigma}^2_1$, can be incorporated in a Stage 2 model as a likelihood term, on an appropriate scale:
$$
\hat{y}_1 \sim N(\theta, \hat{\sigma}^2_1)
$$ for some parameter $\theta$ in the Stage 2 model informed by the Stage 1 estimate. This approach has been shown to be an approximation to Markov melding with ``product of experts'' pooling \citep{goudie2019joining}.

Modular approaches also support judgements of selective trust in sub-models, as indicated by the name ``modularisation'' in the first appearance of a ``cut'' posterior in the literature \citep{LiuEtAl2009}. This approach `cuts' feedback from one module to another, i.e. preventing less reliable sub-models from influencing more trusted ones while allowing the reverse, providing robustness against sub-model misspecification \citep{plummer2015cuts,carmona2022scalable,yu2023variational}. \citet{liu2025general} provide a general framework for deciding how to partition and restrict information flow between sub-models. As with Markov melding, however, practical implementation remains a challenge, since it involves a potentially computationally expensive multiple imputation approach for each MCMC sample \citep{plummer2015cuts}.

\subsubsection{Ensembling independent models}\label{sec:ensembling}

When multiple models target the same estimand, ensemble methods such as Bayesian stacking \citep{yao2018using}, model averaging \citep{hoeting1999bayesian}, or meta-analytical pooling \citep{jackson2011multivariate} can be used to combine estimates.
These approaches are related to model comparison, as discussed in Section \ref{sec:spec-validate}.
They differ from approaches like Markov melding in that they combine independent model outputs rather than partitioning a joint model into connected sub-models. Our choice of ensemble strategy should reflect the research objective and the relationship between the models and the underlying data. When models are fitted on the same dataset, ensemble weights can be based on predictive performance, posterior precision, or model credibility, with Bayesian stacking or model averaging providing principled frameworks for combining the shared estimand \citep{yao2018using}. In contrast, when models are fitted on different datasets targeting the same estimand, traditional model averaging is less formally justified, as marginal likelihoods cannot be compared across disjoint data. In such cases, posterior pooling or stacking on a common predictive task (e.g., predicting future case counts or other observable outcomes implied by the estimand) can be used, or weights can be assigned based on the reliability or relevance of each data source. Ensemble methods offer practical advantages when joint modelling proves infeasible, enabling collaboration between research teams and potentially improving robustness over single models.
The European COVID-19 Forecast Hub demonstrated performance gains from simple median aggregation \citep{sherratt2021exploring}, whilst the UK's SPI-M consensus used random-effects meta-analysis to pool $R_t$ estimates from multiple academic groups \citep{manley2024combining}.

However, important limitations constrain ensemble approaches for infectious disease modelling.
When models make different assumptions about latent quantities, apparent heterogeneity may reflect definitional differences rather than true uncertainty about the estimand \citep{brockhaus2023why}.
Temporal alignment becomes particularly problematic for latent targets where the underlying definition remains ambiguous, with models using different reference points, generation time distributions, or reporting delay assumptions \citep{brockhaus2023why}.
These limitations can lead to estimates that are less timely and/or less precise, with lower utility for decision makers than the component models \citep{manley2024combining}.
Combining outputs at the prediction level discards information about data conflicts and model disagreements that would be preserved in joint modelling approaches.
Weighted ensemble methods often show limited improvement over simple aggregation in nowcasting applications \citep{sherratt2021exploring}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/integration choices decision tree.drawio.pdf}
    \caption{\textbf{Decision tree for integration choices when integrating multiple data sources in infectious disease model inference.} The flowchart guides researchers through key decision points to select appropriate integration strategies.}
    \label{fig:integration}
\end{figure}

\section{Workflow Management}

\subsection{Outbreak Evolution and Workflow Iteration}  \label{sec:outbreak}
% Lead Sam Abbott
Two types of changes during outbreaks require revisiting this workflow.
First, new information emerges about pathogen characteristics, surveillance capabilities, and population dynamics that may necessitate updating model assumptions and data integration choices \citep{mccaw2023role}. Substantial new data sources may become available, such as genomic surveillance or serological surveys; or improved understanding of pathogen biology may change our disease process model assumptions \citep{knock2021key}. 
Second, research questions evolve as outbreak priorities shift from initial detection and characterisation towards intervention evaluation and long-term planning, altering target estimands.
When these changes occur, we iterate through the workflow, revisiting earlier decisions, rather than developing entirely new models.

The modular approach we advocate enables updating specific workflow components whilst preserving elements from previous iterations.
For example, observation models developed for case surveillance can often be reused when adding death reporting, whilst process models for transmission dynamics may require updating when new variants emerge.
This incremental approach reduces development time and maintains continuity across evolving outbreak contexts.

These steps are often carried out on an ad-hoc basis by teams during outbreaks but we think that a more formal approach that tracks decisions is likely to improve the quality of models produced and be less resource-intensive ultimately.
Establishing regular review points to assess whether workflow iteration is needed is likely to be beneficial, with more regular reviews during acute outbreak phases.
Each iteration should document what has changed, why updates are necessary, and how new approaches build upon previous work.
Having this documentation supports both real-time decision-making and retrospective evaluation of analytical approaches.

\subsection{Reporting} \label{sec:reporting}
% Lead Sam Abbott
Complex multi-source models require clear documentation to enable evaluation and reproduction.
Without structured reporting, even well-designed workflows are challenging to assess or improve.
Reporting decisions made at each stage of the workflow is essential, including rationales for data source selection, integration method choices, model structure assumptions, and validation procedures undertaken.
Making analyses public at all stages builds trust, though confidential data requires careful management \citep{Abbott2021-delta, Abbott2022-prevalence}. 
We recommend sharing code and data alongside results because it enables verification, accelerates progress, and it is just as much a part of the analysis as the manuscript.
Not sharing code, sharing it on request, or only once a paper is in a peer-reviewed journal, all create barriers to assessing data integration workflows.
Where barriers to transparent reporting exist, it is always better to report as much as possible.
Mathematical specifications, validation code, and synthetic examples all contribute to transparency even when data cannot be shared, e.g. helping readers understand model behaviour \citep{Mellor2025-norovirus}.
We also recommend visual schematics of the workflow used and the model architecture.

\section{Case Studies}
% Lead: Michael Plank

We demonstrate our iterative workflow through four progressive case study schematics, each building complexity whilst showing different integration choices and methodological considerations.
All case studies share a common estimand -- the time-varying reproduction number ($R_t$) -- while the final case study has the overdispersion parameter ($k$) as an additional estimand.

The progression between case studies illustrates key principles relating to the characterisation of different data sources. Case study 1 uses a single data source, reported case counts. Case study 2 combines case data with data on deaths, enabling some of the limitations of each single data source to be overcome. Case study 3 adds wastewater data as a third data source that is independent of surveillance data biases but comes with its own drawbacks. Case studies 1--3 all deal with aggregated data that cannot provide information about individual variability. Case study 4 combines case count data with individual-level data on transmission pairs to enable direct overdispersion estimation without distributional assumptions.
To support these case studies, we first give an example of our suggested data review approach.

\subsection{Data Sources and Characteristics}

We designed and completed a questionnaire based on the key data characteristics we suggested (Section \ref{sec:datareview}). We first developed targeted questions aligned with each characteristic to facilitate systematic and accessible characterisation. We designed these questions (see Supplementary Material S2) to support both qualitative and quantitative evaluation across data sources. We then completed the survey for a range of data sources (Supplementary Material S2). Here, we focus on the data sources used in our case studies: time series of confirmed cases, deaths, wastewater surveillance data, and transmission-paired observations.

We found that data sources differed across all six characteristics, with substantial variation in responses reflecting differences in subjective interpretation, context, and experience (Figure \ref{survey_responses}, Table \ref{tab:survey_results}). For \textbf{Basic metadata}, we felt that 83\% of confirmed case collections served clinical management, while all wastewater collections served non-clinical purposes. Under \textbf{Scope}, we thought that confirmed case data usually targeted the whole population (100\%),  and wastewater datasets were split between geographic structure (40\%) and whole population (60\%). We considered both sources to have high geographic coverage (mean: 4.5 [range: 4-5] for cases; 4 [3-5] for wastewater (where 1 is low and 5 is high)). For \textbf{Resolution}, we thought that confirmed case datasets were a mix of aggregated (33\%) and individual-level data (67\%). In contrast, we saw wastewater data as entirely aggregated (100\%), though our responses diverged on stratification and reporting frequency. All authors thought that all datasets included temporal information, but that only 50\% of case datasets included spatial information data. \textbf{Data quality} assessments suggested we perceived similar test sensitivity (mean: 4 [range: 3-5] for cases; 3.8 [3-5] for wastewater), but we differed in perceived population representativeness (4.3 [3-5] for cases; 3 [1-4] for wastewater). We thought both sources informed key epidemiological quantities with varying directness and notable disagreement under \textbf{Data utility}: we were split on the role of case data for informing prevalence between directly (33\%), indirectly (33\%), or not informed (33\%), while we all saw wastewater data as indirectly informing prevalence (100\%). Under \textbf{Practical considerations}, we perceived scalability as predominantly linear for both sources (67\% for cases; 80\% for wastewater), but our perceptions of sustainability differed markedly (1 for all case datasets; 3.3 [2-5] for wastewater). Overall, we saw greater feasibility for long-term wastewater surveillance. The variation in our views highlights the importance of acknowledging context-specific practices when evaluating data sources, while demonstrating that the six core characteristics provide a systematic framework for context-sensitive assessment of their strengths and limitations.

{\Fontvi
% \usepackage{color}
% \usepackage{tabularray}
\definecolor{Shark}{rgb}{0.125,0.129,0.141}
\definecolor{Alto}{rgb}{0.827,0.827,0.827}
\begin{longtblr}[
  caption = {Categorical survey responses for confirmed time series (6 responses), wastewater (5 responses), deaths (XX responses), and transmission pairs (XX responses).},
 label = {tab:survey_results},
  entry = none,
]{
  width = \linewidth,
  colspec = {Q[129]Q[342]Q[163]Q[96]Q[65]Q[60]Q[60]},
  cell{1}{3} = {c},
  cell{1}{4} = {c},
  cell{1}{6} = {c=2}{0.12\linewidth},
  cell{2}{1} = {fg=Shark},
  cell{2}{3} = {c},
  cell{2}{4} = {c},
  cell{2}{6} = {c=2}{0.12\linewidth},
  cell{3}{3} = {c},
  cell{3}{4} = {c},
  cell{3}{6} = {c=2}{0.12\linewidth},
  cell{4}{3} = {c},
  cell{4}{4} = {c},
  cell{4}{6} = {c=2}{0.12\linewidth},
  cell{5}{3} = {c},
  cell{5}{4} = {c},
  cell{5}{6} = {c=2}{0.12\linewidth},
  cell{6}{3} = {c},
  cell{6}{4} = {c},
  cell{6}{6} = {c=2}{0.12\linewidth},
  cell{7}{3} = {c},
  cell{7}{4} = {c},
  cell{7}{6} = {c=2}{0.12\linewidth},
  cell{8}{3} = {c},
  cell{8}{4} = {c},
  cell{8}{6} = {c=2}{0.12\linewidth},
  cell{9}{3} = {c},
  cell{9}{4} = {c},
  cell{9}{6} = {c=2}{0.12\linewidth},
  cell{10}{3} = {c},
  cell{10}{4} = {c},
  cell{10}{6} = {c=2}{0.12\linewidth},
  cell{11}{3} = {c},
  cell{11}{4} = {c},
  cell{11}{6} = {c=2}{0.12\linewidth},
  cell{12}{3} = {c},
  cell{12}{4} = {c},
  cell{12}{6} = {c=2}{0.12\linewidth},
  cell{13}{3} = {c},
  cell{13}{4} = {c},
  cell{13}{6} = {c=2}{0.12\linewidth},
  cell{14}{3} = {c},
  cell{14}{4} = {c},
  cell{14}{6} = {c=2}{0.12\linewidth},
  cell{15}{3} = {c},
  cell{15}{4} = {c},
  cell{15}{6} = {c=2}{0.12\linewidth},
  cell{16}{3} = {c},
  cell{16}{4} = {c},
  cell{16}{6} = {c=2}{0.12\linewidth},
  cell{17}{3} = {c},
  cell{17}{4} = {c},
  cell{17}{6} = {c=2}{0.12\linewidth},
  cell{18}{3} = {c},
  cell{18}{4} = {c},
  cell{18}{6} = {c=2}{0.12\linewidth},
  cell{19}{3} = {c},
  cell{19}{4} = {c},
  cell{19}{6} = {c=2}{0.12\linewidth},
  cell{20}{1} = {fg=Shark},
  cell{20}{3} = {c},
  cell{20}{4} = {c},
  cell{20}{6} = {c=2}{0.12\linewidth},
  cell{21}{3} = {c},
  cell{21}{4} = {c},
  cell{21}{6} = {c=2}{0.12\linewidth},
  cell{22}{3} = {c},
  cell{22}{4} = {c},
  cell{22}{6} = {c=2}{0.12\linewidth},
  cell{23}{3} = {c},
  cell{23}{4} = {c},
  cell{23}{6} = {c=2}{0.12\linewidth},
  cell{24}{3} = {c},
  cell{24}{4} = {c},
  cell{24}{6} = {c=2}{0.12\linewidth},
  cell{25}{3} = {c},
  cell{25}{4} = {c},
  cell{25}{6} = {c=2}{0.12\linewidth},
  cell{26}{3} = {c},
  cell{26}{4} = {c},
  cell{26}{6} = {c=2}{0.12\linewidth},
  cell{27}{3} = {c},
  cell{27}{4} = {c},
  cell{27}{6} = {c=2}{0.12\linewidth},
  cell{28}{3} = {c},
  cell{28}{4} = {c},
  cell{28}{6} = {c=2}{0.12\linewidth},
  cell{29}{3} = {c},
  cell{29}{4} = {c},
  cell{29}{6} = {c=2}{0.12\linewidth},
  cell{30}{3} = {c},
  cell{30}{4} = {c},
  cell{30}{6} = {c=2}{0.12\linewidth},
  cell{31}{3} = {c},
  cell{31}{4} = {c},
  cell{31}{6} = {c=2}{0.12\linewidth},
  cell{32}{3} = {c},
  cell{32}{4} = {c},
  cell{32}{6} = {c=2}{0.12\linewidth},
  cell{33}{3} = {c},
  cell{33}{4} = {c},
  cell{33}{6} = {c=2}{0.12\linewidth},
  cell{34}{3} = {c},
  cell{34}{4} = {c},
  cell{34}{6} = {c=2}{0.12\linewidth},
  cell{35}{3} = {c},
  cell{35}{4} = {c},
  cell{35}{6} = {c=2}{0.12\linewidth},
  cell{36}{3} = {c},
  cell{36}{4} = {c},
  cell{36}{6} = {c=2}{0.12\linewidth},
  cell{37}{3} = {c},
  cell{37}{4} = {c},
  cell{37}{6} = {c=2}{0.12\linewidth},
  cell{38}{3} = {c},
  cell{38}{4} = {c},
  cell{38}{6} = {c=2}{0.12\linewidth},
  cell{39}{3} = {c},
  cell{39}{4} = {c},
  cell{39}{6} = {c=2}{0.12\linewidth},
  cell{40}{3} = {c},
  cell{40}{4} = {c},
  cell{40}{6} = {c=2}{0.12\linewidth},
  cell{41}{3} = {c},
  cell{41}{4} = {c},
  cell{41}{6} = {c=2}{0.12\linewidth},
  cell{42}{3} = {c},
  cell{42}{4} = {c},
  cell{42}{6} = {c=2}{0.12\linewidth},
  cell{43}{3} = {c},
  cell{43}{4} = {c},
  cell{43}{6} = {c=2}{0.12\linewidth},
  cell{44}{3} = {c},
  cell{44}{4} = {c},
  cell{44}{6} = {c=2}{0.12\linewidth},
  cell{45}{3} = {c},
  cell{45}{4} = {c},
  cell{45}{6} = {c=2}{0.12\linewidth},
  cell{46}{3} = {c},
  cell{46}{4} = {c},
  cell{46}{6} = {c=2}{0.12\linewidth},
  cell{47}{3} = {c},
  cell{47}{4} = {c},
  cell{47}{6} = {c=2}{0.12\linewidth},
  cell{48}{3} = {c},
  cell{48}{4} = {c},
  cell{48}{6} = {c=2}{0.12\linewidth},
  cell{49}{3} = {c},
  cell{49}{4} = {c},
  cell{49}{6} = {c=2}{0.12\linewidth},
  cell{50}{3} = {c},
  cell{50}{4} = {c},
  cell{50}{6} = {c=2}{0.12\linewidth},
  cell{51}{3} = {c},
  cell{51}{4} = {c},
  cell{51}{6} = {c=2}{0.12\linewidth},
  cell{52}{3} = {c},
  cell{52}{4} = {c},
  cell{52}{6} = {c=2}{0.12\linewidth},
  cell{53}{3} = {c},
  cell{53}{4} = {c},
  cell{53}{6} = {c=2}{0.12\linewidth},
  cell{54}{3} = {c},
  cell{54}{4} = {c},
  cell{54}{6} = {c=2}{0.12\linewidth},
  cell{55}{3} = {c},
  cell{55}{4} = {c},
  cell{55}{6} = {c=2}{0.12\linewidth},
  cell{56}{3} = {c},
  cell{56}{4} = {c},
  cell{56}{6} = {c=2}{0.12\linewidth},
  cell{57}{3} = {c},
  cell{57}{4} = {c},
  cell{57}{6} = {c=2}{0.12\linewidth},
  cell{58}{3} = {c},
  cell{58}{4} = {c},
  cell{58}{6} = {c=2}{0.12\linewidth},
  cell{59}{3} = {c},
  cell{59}{4} = {c},
  cell{59}{6} = {c=2}{0.12\linewidth},
  cell{60}{3} = {c},
  cell{60}{4} = {c},
  cell{60}{6} = {c=2}{0.12\linewidth},
  cell{61}{3} = {c},
  cell{61}{4} = {c},
  cell{61}{6} = {c=2}{0.12\linewidth},
  cell{62}{3} = {c},
  cell{62}{4} = {c},
  cell{62}{6} = {c=2}{0.12\linewidth},
  cell{63}{3} = {c},
  cell{63}{4} = {c},
  cell{63}{6} = {c=2}{0.12\linewidth},
  cell{64}{3} = {c},
  cell{64}{4} = {c},
  cell{64}{6} = {c=2}{0.12\linewidth},
  cell{65}{3} = {c},
  cell{65}{4} = {c},
  cell{65}{6} = {c=2}{0.12\linewidth},
  cell{66}{3} = {c},
  cell{66}{4} = {c},
  cell{66}{6} = {c=2}{0.12\linewidth},
  cell{67}{3} = {c},
  cell{67}{4} = {c},
  cell{67}{6} = {c=2}{0.12\linewidth},
  cell{68}{3} = {c},
  cell{68}{4} = {c},
  cell{68}{6} = {c=2}{0.12\linewidth},
  cell{69}{3} = {c},
  cell{69}{4} = {c},
  cell{69}{6} = {c=2}{0.12\linewidth},
  cell{70}{3} = {c},
  cell{70}{4} = {c},
  cell{70}{6} = {c=2}{0.12\linewidth},
  cell{71}{3} = {c},
  cell{71}{4} = {c},
  cell{71}{6} = {c=2}{0.12\linewidth},
  hline{1,73} = {-}{0.08em},
  hline{2,8,20,43,68,72} = {-}{},
  hline{3-7,9-19,21-42,44-67,69-71} = {2-7}{Alto},
}
\textbf{Core characteristic} & \textbf{Sub-characteristic} & {\textbf{Confirmed cases time series}\\N = 6\textit{\textsuperscript{1}}} & {\textbf{Wastewater}\\N = 5\textit{\textsuperscript{1}}} & {\textbf{Deaths}\\N=?~} & {\textbf{Transmission pairs~}\\N=?\textbf{}} & \\
{\textbf{Basic}\\\textbf{~meta-data}} & \textbf{Study design} &  &  &  &  & \\
 & ~~~~Routine surveillance & 6 (100\%) & 3 (60\%) &  &  & \\
 & ~~~~Sentinel surveillance & 0 (0\%) & 2 (40\%) &  &  & \\
 & \textbf{Primary purpose} &  &  &  &  & \\
 & ~~~~Clinical management & 5 (83\%) & 0 (0\%) &  &  & \\
 & ~~~Other & 1(17\%) & 5 (10\%) &  &  & \\
\textbf{\textbf{Scope}} & \textbf{Target population} &  &  &  &  & \\
 & ~~~~Geographic structure & 0 (0\%) & 2 (40\%) &  &  & \\
 & ~~~~Whole population & 6 (100\%) & 3 (60\%) &  &  & \\
 & \textbf{Stratification/covariates (except spatial-temporal see Resolution section)} &  &  &  &  & \\
 & ~~~~Demographic & 1 (17\%) & 0 (0\%) &  &  & \\
 & ~~~~Demographic, Clinical & 3 (50\%) & 0 (0\%) &  &  & \\
 & ~~~~None & 2 (33\%) & 5 (100\%) &  &  & \\
 & \textbf{Collection type} &  &  &  &  & \\
 & ~~~~Routine & 6 (100\%) & 5 (100\%) &  &  & \\
 & \textbf{If triggered, potential triggers} &  &  &  &  & \\
 & ~~~~New variant/pathogen detected & 1 (100\%) & 0 (NA\%) &  &  & \\
 & ~~~~Unknown & 5 & 5 &  &  & \\
\textbf{Resolution} & \textbf{Data aggregation} &  &  &  &  & \\
 & ~~~~Aggregated & 2 (33\%) & 5 (100\%) &  &  & \\
 & ~~~~Individual & 4 (67\%) & 0 (0\%) &  &  & \\
 & \textbf{Temporal data} &  &  &  &  & \\
 & ~~~~Yes & 6 (100\%) & 5 (100\%) &  &  & \\
 & \textbf{Collection frequency} &  &  &  &  & \\
 & ~~~~Continuously & 1 (17\%) & 1 (20\%) &  &  & \\
 & ~~~~Daily & 4 (67\%) & 0 (0\%) &  &  & \\
 & ~~~~Multiple times a week & 0 (0\%) & 3 (60\%) &  &  & \\
 & ~~~~Weekly & 1 (17\%) & 1 (20\%) &  &  & \\
 & \textbf{Reporting frequency} &  &  &  &  & \\
 & ~~~~Daily & 3 (50\%) & 0 (0\%) &  &  & \\
 & ~~~~Multiple times a week & 0 (0\%) & 2 (40\%) &  &  & \\
 & ~~~~Weekly & 3 (50\%) & 3 (60\%) &  &  & \\
 & \textbf{Time period covered} &  &  &  &  & \\
 & ~~~~all the time & 0 (0\%) & 1 (20\%) &  &  & \\
 & ~~~~Continuous & 6 (100\%) & 3 (60\%) &  &  & \\
 & ~~~~Endemic & 0 (0\%) & 1 (20\%) &  &  & \\
 & \textbf{Spatial data} & 3 (50\%) & 5 (100\%) &  &  & \\
 & \textbf{Spatial resolution} &  &  &  &  & \\
 & ~~~~Local & 1 (17\%) & 5 (100\%) &  &  & \\
 & ~~~~National & 2 (33\%) & 0 (0\%) &  &  & \\
 & ~~~~Regional & 3 (50\%) & 0 (0\%) &  &  & \\
\textbf{Data utility} & \textbf{Quantities informed [Basic reproduction number]} &  &  &  &  & \\
 & ~~~~Direct & 0 (0\%) & 1 (20\%) &  &  & \\
 & ~~~~Indirect & 5 (83\%) & 0 (0\%) &  &  & \\
 & ~~~~Not informed & 1 (17\%) & 4 (80\%) &  &  & \\
 & Qu\textbf{antities informed [Time-varying reproduction number]} &  &  &  &  & \\
 & ~~~~Direct & 4 (67\%) & 1 (20\%) &  &  & \\
 & ~~~~Indirect & 2 (33\%) & 4 (80\%) &  &  & \\
 & \textbf{Quantities informed [Time-varying / basic reproduction number ratio]} &  &  &  &  & \\
 & ~~~~Direct & 0 (0\%) & 1 (20\%) &  &  & \\
 & ~~~~Indirect & 5 (83\%) & 0 (0\%) &  &  & \\
 & ~~~~Not informed & 1 (17\%) & 4 (80\%) &  &  & \\
 & \textbf{Quantities informed [Incidence]} &  &  &  &  & \\
 & ~~~~Direct & 3 (50\%) & 0 (0\%) &  &  & \\
 & ~~~~Indirect & 3 (50\%) & 4 (80\%) &  &  & \\
 & ~~~~Not informed & 0 (0\%) & 1 (20\%) &  &  & \\
 & \textbf{Quantities informed [Prevalence]} &  &  &  &  & \\
 & ~~~~Direct & 2 (33\%) & 0 (0\%) &  &  & \\
 & ~~~~Indirect & 2 (33\%) & 5 (100\%) &  &  & \\
 & ~~~~Not informed & 2 (33\%) & 0 (0\%) &  &  & \\
 & \textbf{Quantities informed [Heterogeneity in transmission (e.g. superspreading)]} &  &  &  &  & \\
 & ~~~~Indirect & 4 (67\%) & 0 (0\%) &  &  & \\
 & ~~~~Not informed & 2 (33\%) & 5 (100\%) &  &  & \\
 & \textbf{Quantities informed [Drivers of transmission]} &  &  &  &  & \\
 & ~~~~Indirect & 3 (50\%) & 0 (0\%) &  &  & \\
 & ~~~~Not informed & 3 (50\%) & 5 (100\%) &  &  & \\
\textbf{\textbf{Practical considerations}} & \textbf{Scalability} &  &  &  &  & \\
 & ~~~~Independent & 0 (0\%) & 4 (80\%) &  &  & \\
 & ~~~~Linearly & 4 (67\%) & 0 (0\%) &  &  & \\
 & ~~~~Sub-exponentially & 2 (33\%) & 1 (20\%) &  &  & \\
 & \textit{\textsuperscript{1}}~n (\%) &  &  &  &  & 
\end{longtblr}
}

%
\begin{figure}[H] 
\includegraphics[width=.97\linewidth]{figures/survey_responses.png}
\centering
\caption{ Selected data characteristics from the survey, showing responses related to confirmed case time series (6 responses) and wastewater surveillance (5 responses). The y-axis represents specific data characteristics, while the x-axis indicates the response range on a 5-point scale, where 1 corresponds to a low level and 5 to a high level. The lower triangles present the average response for each data characteristic.}
\label{survey_responses}
\end{figure}



\subsection{Case Study 1: Single-Source Baseline (Cases Only)}


In this case study, we showcase the workflow and explore some of the assumptions and modelling and fitting choices underlying estimation of $R_t$ from a single time series of case incidence.

\textbf{Workflow Demonstration:}
\begin{enumerate}
    \item \textbf{Research question and target estimands:} The research question for this case study (and case studies 2 and 3) is: what is the time-varying reproduction number $R_t$ during this outbreak period?

    \item \textbf{Process DAG development:} We begin with a flexible branching process model for the number of people $N_{i,t}$ newly infected by individual $i$ on day $t$:
    \begin{equation} \label{eq:individual_level}
         N_{i,t} \sim \mathrm{Poiss} \left( Y_i R_t w_{t-T_i} \right)
    \end{equation} 
    Here, $T_i$ is the infection time of individual $i$, $Y_i$ is the relative transmission rate of individual $i$ (which may capture a combination of biological and social factors), and $w_s$ is the relative infectiousness $s$ days after infection normalised such that $\sum_{s=1}^\infty w_s=1$. This explicitly models the full transmission tree of the epidemic whilst remaining relatively parsimonious. We assume that the infectiousness profile $w_s$ does not change over time, but note that we would need to revisit this assumption if there was evidence this had changed, for example due to biological changes in the pathogen, changes in contact patterns, or case isolation measures.
    

\item \textbf{Data source selection:} Case incidence data is typically readily available with very frequent updates and relatively good timeliness (good scope and resolution), though this can depend on lab processing and reporting timelines. These factors make case data an obvious choice for trying to model time-varying transmission parameters. However, case counts are an imperfect and often biased proxy for infection incidence and are sensitive to testing patterns, which can vary over time (data quality issues) -- see Table \ref{tab:survey_results} and Figure \ref{survey_responses}. 


\item \textbf{Observation DAG construction:} The observation model relates the observed data (here daily case incidence) with underlying latent variables (here, daily infection incidence). 
This requires us to make assumptions about case ascertainment (what fraction of infections are detected cases), reporting delays (time between infection and a case being detected and reported), and random noise (typically used to capture other things not explicitly specified in the model).

There are at least  $2^3=8$ possible observation models to consider, which either ignore or include each of these three aspects.
We refer to these models as $O_{000}$ (for the simplest observation model not modelling any of the above) to $O_{111}$ (for the most comprehensive accounting for all three observation features). We represent the $O_{111}$ model by the following equation for the observed variable $C_t$, representing the number of reported cases on day $t$:
\begin{equation} \label{eq:cases}
    C_t \sim \mathrm{Poiss}\left( \sum_{s=1}^t M_{t-s,s}\right)
\end{equation}
where $M_{t,s}$ is the number of people infected on day $t$ and reported as a case $s$ days later, given by
\begin{equation}
    M_{t,.} \sim \mathrm{TruncatedMultinomial}\left( I_t, p_c \boldsymbol{\alpha} \right) 
\end{equation}
Here $I_t=\sum_i N_{i,t}$ is the total number of new infections on day $t$, $p_c$ is the probability an infection is reported as a case (i.e. case ascertainment ratio), which we assume to be constant over time, and $\boldsymbol{\alpha}$ is a vector containing the probability mass function of the distribution of time from infection to reporting. We choose to use the most comprehensive observation model $O_{111}$ as it is important to account for all three factors.



\item \textbf{Refine the model DAGs:} Since we only have aggregate-level data on the number of reported cases and no individual-level data about the transmission tree, we recognise that individual heterogeneity in transmission rates (represented by the variable $Y_i$) will not be identifiable. We therefore simplify the model by assuming that $Y_i=1$ for all individuals. With this simplification, the model for the number of new infections $I_t$ on day $t$ reduces to
    \begin{equation} 
        I_t \sim \mathrm{Poiss}\left( R_t \sum_{s=1}^\infty I_{t-s}w_s  \right)
    \end{equation}
    This is an example of the well-known renewal process model. This is a popular model for $R_t$ estimation as it captures fundamental features of the transmission process (newly infected individuals at time $t$ transmit the pathogen to other individuals some time later), without making additional assumptions, for example about the size of the susceptible population or the strength and duration of infection-acquired immunity. This provides a simple but flexible model to which additional complexity can be added in subsequent iterations of the workflow.
    There are other valid assumptions we might wish to make depending on other workflow considerations. Here, we consider three variants of the renewal process model with differing models for the variance in daily infection incidence $I_t$:
    \begin{itemize}
    \item[$P_1$.] Deterministic model for daily infection incidence (no variance).
    \begin{equation} \label{eq:infections_P1}
        I_t = R_t \sum_{s=1}^\infty I_{t-s}w_s 
    \end{equation}
    \item[$P_2$.] Poisson-distributed daily infection incidence.
        \begin{equation} \label{eq:infections_P2}
        I_t \sim \mathrm{Poiss}\left( R_t \sum_{s=1}^\infty I_{t-s}w_s  \right)
    \end{equation}
    \item[$P_3$.] Negative binomially distributed daily infection incidence (larger variance than Poisson). 
            \begin{equation} \label{eq:infections_P3}
        I_t \sim \mathrm{NegBin}\left( R_t \sum_{s=1}^\infty I_{t-s}w_s, k  \right)
    \end{equation}
    where $k$ is the dispersion parameter. 
    \end{itemize} 
Figure \ref{fig:case_study_visual} shows the process DAGs and observation DAGs for all potential models. Here, $P_1$ is unlikely to be realistic as there will almost always be some source of random variability in daily infection incidence. The choice between $P_2$ and $P_3$ could depend on whether superspreading is known to be a major factor in the transmission dynamics. Since $P_3$ contains $P_2$ in the limiting case $k\to\infty$, we choose to use $P_3$ in this example (see Figure \ref{fig:case_study_diagram}).
Importantly, the negative binomial distribution in $P_3$ captures both potential biological overdispersion (superspreading) and unmodelled variance in the data. 


\item \textbf{Modularise the DAGs:} Here we do not need to do any modularisation as we are only dealing with a single data source.

\item \textbf{Inference and implementation choices:} Implementation requires selection of one process model, one observation model and one fitting method. The choice of fitting method depends on the analytical tractability of the combined process and observation model, computational complexity and speed of computation relative to the time available, and identifiability issues (see Section \ref{sec:fitting}). 
For our chosen combination of models ($P_3$-$O_{111}$), we recommend using NUTS but only if we approximate the discrete observation models (e.g. EpiNow2 \citep{abbott2020estimating}), or if we do not want to do this we would need to use an alternative method such as PMCMC. If we needed a simpler, more computationally efficient fitting method, we could return to the ``Refine models'' stage and select a simpler model. For the simplest model combination with no underreporting, delays or observation noise ($P_1$-$O_{000}$), we can calculate $R_t$ directly for a given choice of generation time distribution by rearranging Eq. \eqref{eq:infections_P1}. This is likely to give biased and highly noisy estimates in practice and does not provide any uncertainty quantification. For the next-simplest model ($P_2$-$O_{000}$), we can calculate an analytical posterior distribution for $R_t$  from a conjugate gamma prior, which is fast and computationally efficient. This is implemented in the widely used EpiEstim method \citep{cori2013new}. 

\item \textbf{Model specification and validation:} Model specification requires a prior for $R_t$ as well as specification of the profile of infectiousness over time $w_t$, the reporting probability $p_c$, the distribution of time from infection to reporting $\boldsymbol{\alpha}$ and (for process model $P_3$) the dispersion coefficient $k$. The simplest approach is to take these as fixed parameters, which corresponds to an infinite-strength prior. This is unlikely to ever be appropriate but may be required for computational tractability. However, a better approach in the presence of uncertainty is to define a prior over these parameters based on domain expertise. In the case of probability distributions $\boldsymbol{w}$ and $\boldsymbol{\alpha}$, this could take the form of a parametric distribution assumption, e.g. a discretised gamma \citep{charniga2024best,park2024estimating} or other non-negative-integer valued distribution with specified priors for the mean and variance. 

Different modelling approaches have different strengths and limitations. For example, simple approaches are fast to implement, but ignore important factors in the data observation process. These may be useful for a quick initial estimate, but should be revisited as needed. In this case study, $R_t$ estimates may be insensitive to underreporting provided the reporting fraction is constant over the period of interest. Furthermore, any change in reporting fraction will not be identifiable from a case time series alone (if $R_t$ varies over time same time scale), so it would not be sensible to attempt to estimate this in this example. However, reporting delays can be substantial and will affect both the magnitude and timing of $R_t$ estimates. We can account for this by selecting an appropriate observation model, at the cost of needing to move to a more computationally costly fitting method. Ultimately, the most appropriate option will depend on the balance between need for timely estimates (e.g. for modelling an outbreak in real-time) and the realism of the associated assumptions. 
Finally, a common weakness of all the models outlined above is that they assume the profile of infectiousness over time is constant. Some approaches (e.g. some versions of EpiEstim, and EpiNow2 \citep{abbott2020estimating}) extend the approach to relax this assumption.
 
\item \textbf{Data integration choices:} As there is only one data source in this case study, we do not need to make any decisions about what and how to integrate multiple sources. In a scenario where multiple groups use different methods to produce estimates of the target estimand (in this case $R_t$) based on the same data, we could integrate the results by ensembling them into a combined estimate \citep{maishman2022statistical,manley2024combining}. In this context, it is desirable to align the assumptions made by different models, for example about the generation time distribution or the temporal smoothness in $R_t$, to ensure that estimates from the different models are comparable \citep{brockhaus2023why}.

 \end{enumerate}
 


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/case_study_puzzle_complete.png}
    \caption{\textbf{Modular representation for infectious disease modelling showing the interchangeable components of process models, observation models, integration approaches, and fitting methods.} Process models include deterministic ($P_1$), stochastic Poisson ($P_2$), and negative binomial ($P_3$) formulations relating reproduction number ($R_t$) to number of new infections ($I_t$) with generation time ($GT$) distributions. $P_3$ incorporates overdispersion parameter $k$ to caption extra-Poisson variation. Observation models range from direct observation ($O_{000}$) to complex hierarchical structures ($O_{111}$) that model the relationship between $I_t$ and (observed) case incidence through underreporting, reporting delays and measurement uncertainty. The jigsaw puzzle metaphor illustrates how different modelling components can be combined flexibly, with inference method including MCMC, VI, ABC, etc. Integration approaches span from full joint modelling to ensembling independent models, enabling researchers to select optimal combinations based on data availability and research objectives. }
    \label{fig:case_study_visual}
\end{figure}


\subsection{Case Study 2: Two-Source Integration (Cases and Deaths)}

More data sources often become available later in an outbreak. For example, early in an outbreak, surveillance data may be restricted to reported cases due to the delay from infection to development of clinically severe disease. However, data subsequently became available on other events such as hospitalisation, ICU admission or death. In this case study, we model time-varying transmission rates using two data sources: reported cases and deaths. We illustrate the process and challenges of adding an additional data stream and explore how this can improve $R_t$ estimation and reduce assumption dependence.

\begin{enumerate}
   \item \textbf{Research question and target estimands:}  In this case study, we target the same estimand as in case study 1 ($R_t$). 
   
    \item \textbf{Process DAG development:} We consider the same process DAG as in case study 1 (see Figure \ref{fig:case_study_diagram}).
        
    \item \textbf{Data source selection:} Data on deaths typically have longer delays (delay from infection to death and potentially from date of death to date of registration), but lower noise (e.g. due to ``day-of-the-week'' reporting effects), higher ascertainment, and less dependence on testing patterns than case incidence data. Combining deaths data with cases can therefore potentially overcome some of the limitations present in each single data source. Based on the data survey (Table \ref{survey_responses}), we expect these data sources to have differing time-varying biases and so to be useful for unpicking the dynamics of the underlying infection time series.
    
    \item \textbf{Observation DAG construction:} Similar to the number of reported cases defined by Eq. \eqref{eq:cases}, the number of deaths $D_t$ that occur on day $t$ depends on the time series of infections up to day $t$, the infection-fatality ratio $p_d$, and the distribution $v_s$ of time ($s$) from infection to death. We represent this in the observation DAG (Figure \ref{fig:case_study_diagram}) and by the following convolution equation \citep{bhatt2023semi}:
    \begin{equation} \label{eq:deaths}
        D_t \sim \mathrm{Poiss}\left(p_d \sum_{s=1}^\infty I_{t-s}v_s \right)
    \end{equation}
    Other models for deaths are possible, for example using a multinomial model for the number of deaths occurring on day $t$ due to infections on day $t-s$ ($s=1,2,\ldots$). However, the Poisson model is a good approximation provided $p_d\ll 1$. 
    Eq. \eqref{eq:deaths} provides a model for the time series of daily deaths $D_t$ conditional on the time series of daily infections $I_t$ and would be coupled with one of Eqs. \eqref{eq:infections_P1}--\eqref{eq:infections_P3}, which describe the dynamics of infections, to produce a joint model for infections and deaths. We begin with the simplest observation model, which assumes that all deaths are recorded immediately, so the observed variable is simply $D^\mathrm{obs}_t=D_t$.
    


\item \textbf{Refine the model DAGs:}       
    Similar to the different choices of observation model for cases, the observation model for deaths could include underreporting, reporting delays, and random noise. The observation model described above corresponds to $O_{000}$. However, in revisiting the data on deaths, we discover that there is sometimes a substantial delay from date of death to date of registration \citep{seaman2022nowcasting}. If we were conducting the analysis retrospectively, we could use model $O_{000}$ for data by date of death (i.e. define $D^\mathrm{obs}_t$ to be the number of deaths that occurred on day $t$). However, for a real-time analysis, this data would be incomplete and it would be important to account for the delay. Hence, we use data by date of registration (i.e. define $D^\mathrm{obs}_t$ to be the number of deaths registered on day $t$) and use model $O_{010}$. This still assumes that all deaths are eventually reported with no additional observation noise (which is reasonable for jurisdictions with comprehensive death records), but accounts for the observation delay via the following equation for the number of deaths registered on day $t$:
\begin{equation}
    D^\mathrm{obs}_t = \sum_{s=0}^\infty M_{t-s,s}
\end{equation}
where $M_{t,s}$ is the number of deaths that occurred on day $t$ and were registered $s$ days later, given by
\begin{equation}
    M_{t,.} \sim \mathrm{Multinomial}\left( D_t, \boldsymbol{\beta}\right) 
\end{equation}
where $\boldsymbol{\beta}$ is a vector containing the probability mass function for the number of days from date of death to date of registration.

 \item \textbf{Modularise the DAGs:} The module for reported cases is the same as in case study 1. Here, we add a second module for deaths (see Figure \ref{fig:case_study_diagram}).

\item \textbf{Inference and implementation choices:} As each submodel and the joint model have a differentiable likelihood, we recommend NUTS for fitting. If some of the submodels had non-differentiable likelihoods, we would recommend using NUTS on the submodels where this was possible, and PMCMC on the remaining submodels and the joint model.

\item \textbf{Model specification and validation:} In addition to the parameters from case study 1, this model requires us to specify the infection fatality ratio $p_d$, the distribution of time from infection to death $\bf{v}$ and from date of death to date to date of registration $\boldsymbol{\beta}$. Again, we recommend using appropriately informative priors where possible to allow for uncertainty. 


    \item \textbf{Data integration choice:} 
       Two different approaches are possible for integration: (1) a joint model including both cases and deaths with a shared latent state $R_t$ \citep{scott2020epidemia}; (2) separate models for cases and deaths, resulting in two estimates for $R_t$ to be combined via a weighted ensemble or Markov melding.
        As outlined above, we recommend fitting separate models to the two time series initially, to understand their behaviour and reveal whether they lead to consistent or conflicting estimates of $R_t$ \citep{sherratt2021exploring}. Where inconsistent results emerge from the two data sources, this could prompt us to refine the model. For example, if $R_t$ estimates show similar trends but shifted in time, we would need to revisit assumptions about delays (return to model specification and validation (Section~\ref{sec:spec-validate})). If $R_t$ estimates are largely consistent, but the case-based estimate shows a transient increase in $R_t$ that does not occur in the deaths-based estimate, this could indicate an increase in case ascertainment (return to refine the model DAGs (Section~\ref{sec:refine-dags}) to include time-varying ascertainment). In case study 1 using data on cases alone, the case ascertainment ratio was non-identifiable and had to be assumed. Including data on deaths, if the infection-fatality ratio is known, means that we could also target the case ascertainment ratio for estimation using a joint model. However, this requires assumptions about the distribution of time from infection to death, demonstrating how additional data sources can shift rather than eliminate the need for modelling assumptions. 
\end{enumerate}

\subsection{Case Study 3: Three-Source Integration (Cases, Deaths and Wastewater)}

Wastewater-based epidemiology has emerged as a complementary data source to traditional disease surveillance methods \citep{keshaviah2023wastewater}. Wastewater surveillance has been used for a range of viral pathogens including norovirus \citep{zheng2024tracking}, poliovirus \citep{whitehouse2024wastewater}, and influenza viruses \citep{zheng2023development}. During the Covid-19 pandemic, wastewater surveillance supported outbreak detection \citep{hewitt2022sensitivity}, situational awareness and forecasting \citep{varkila2023use,jin2024combining}, and multi-strain modelling \citep{dreifuss2025estimated}. 

\begin{enumerate}
   \item \textbf{Research question and target estimands:} In this case study, we include measurements of viral RNA/DNA concentration in wastewater samples as a third data source alongside reported cases and deaths. We explore ways of handling conflicting signals between data sources and incorporating complex observation processes. The target estimand is $R_t$, as in case studies 1 and 2.
   
    \item \textbf{Process DAG development:} We consider the same process DAG as in case studies 1 and 2 (see Figure \ref{fig:case_study_diagram}).

    \item \textbf{Data source selection:} Wastewater data offers population-level information (broad scope) independent of testing biases and changes in testing rates, with moderate timeliness, but requiring environmental expertise and subject to high levels of unexplained variability. It could also be sensitive to unknown changes in the average shedding rate over time and has relatively poor resolution (limited spatial resolution and no information about age or other demographic variables). 

    
    \item \textbf{Observation DAG construction:} 
    Suppose that the average viral RNA/DNA load $W_t$ in wastewater on day $t$, typically quantified in genome copies per person per day, depends on the time series of infections up to day $t$ and the average amount of viral RNA/DNA that an individual sheds into wastewater $s$ days after being infected ($s=1,2,\ldots$). We represent this in the process DAG in Figure \ref{fig:case_study_diagram} and the following equation for $W_t$:
    \begin{equation} \label{eq:wastewater}
        W_t = \frac{c}{N_\mathrm{pop}}\sum_{s=1}^\infty I_{t-s}u_s 
    \end{equation}
    where $c>0$ is a constant representing the average total amount of viral RNA/DNA that an infected individual sheds over the course of their infection, $N_\mathrm{pop}$ is the total population size, and $u_s$ is the average shedding rate $s$ days after infection normalised such that $\sum_{s=1}^\infty u_s=1$. We would couple Eq. \eqref{eq:wastewater} with one of Eqs. \eqref{eq:infections_P1}--\eqref{eq:infections_P3} for infections.
    
   Wastewater samples are typically collected at some cadence from one or more sampling sites, and the concentration of viral RNA/DNA in the samples is quantified via PCR testing. The existence of multiple sites with different catchments populations and non-contemporaneous sampling frequencies complicates the interpretation of quantitative wastewater data, which can be modelled at varying levels of complexity.   
   Suppose that measurements of the wastewater concentration $W^\mathrm{obs}_{j,t}$ are taken from sampling sites $j=1,\ldots, J$ on some subset of days $t$. Similarly to \citep{watson2024jointly}, we assume that these observations are conditionally independent gamma random variables with the same mean $W_t$ and variance $b W_t^2/N_j$, where $N_j$ is the population size in the catchment for sampling site $j$ and $b$ is a variance parameter:
    \begin{equation}
        W^\mathrm{obs}_{j,t} \sim \Gamma\left(\mathrm{mean}= W_t,  \mathrm{var}=\frac{b W_t^2}{N_j} \right)
    \end{equation}

       
\item \textbf{Refine the model DAGs:}  The formulation above assumes that the different catchment areas have identical epidemic dynamics and the only difference between catchments is that those with smaller populations have higher measurement variance. This is a reasonable starting assumption if the outbreak is evenly distributed geographically. However, it assumes that the variance in observed data is primarily due to environmental variation and individual heterogeneity, and hence scales inversely with population size. This ignores the contribution to variability of noise in PCR measurements, which depends on the concentration and therefore on sewage system-specific factors like total flow volume. The relationship between mean and variance in wastewater measurements is likely to be much less direct than for case count data. Furthermore, if different sampling sites reveal different temporal trends, this would indicate the need to refine the model to account for different epidemic dynamics in different regions. This could require revisiting the data source selection (Section~\ref{sec:data-selection}) for an assessment of whether the catchment areas for wastewater sampling are aligned with the geographical stratification (if any) available for reported cases and deaths. More complex observation models could also incorporate other factors such as individual-level and site-level variation, catchment population dynamics, spatial heterogeneity, different sampling methods, and environmental degradation.

\item \textbf{Modularise the DAGs:} The modules for cases and deaths are the same as in case studies 1 and 2 respectively. The module for wastewater data follows the same pattern, with wastewater representing a third process downstream from the daily infection incidence.
    
    \item \textbf{Inference and implementation choices:} As before, we recommend using a gradient-based sampling method such as NUTS where possible. For real-time estimation during an outbreak, this may be impractical, in which case, we might need to consider a method such as SMC to support sequential data assimilation, provided that model parameters are known (informed by expert opinion or previous epidemics) and the focus is primarily on state estimation.  For a joint model, the increased state-space complexity would likely require a method such as PMCMC.

    \item \textbf{Model specification and validation:} In addition to the parameters from the previous two case studies, we need to specify the average shedding rate $c$, the total population size $N_\mathrm{pop}$, the population in each sampled catchment $N_j$, and the shedding profile over time $u_t$. Wastewater data enables $R_t$ estimation independent of shifts in testing patterns, but requires assumptions about the shedding, environmental and sampling processes. Similar to the unknown case ascertainment ratio in case study 1, estimates of $R_t$ will, at sufficiently large incidences, be insensitive to the value of $c$ provided it does not change over time \citep{dreifuss2025estimated}. However, changes in the average shedding rate due to, for example, pathogenic evolution or changes in population immunity, would invalidate this simple model and require a more nuanced approach. 
    
    \item \textbf{Data integration choice:} As in case study 2, we recommend a modular approach with sequential consistency assessment to detect and resolve data source conflicts. For example, a time lag in estimates of $R_t$ from wastewater data alone relative to estimates of $R_t$ from cases alone could suggest misspecification of the shedding rate distribution $u_s$.  It is also useful to compare estimates of latent states between separate models. For example, if estimates of $I_t$ from the cases model and the wastewater model are initially consistent but start to diverge after a certain time, this could indicate that either the case ascertainment ratio $p_c$ or the average shedding rate $c$ has changed. An assessment of which of these explanations is more likely might be informed by the context. For example, a decrease in testing would suggest a drop in case ascertainment is likely, while a vaccine rollout might suggest that a drop in average shedding is likely due to increased immunity. 

    Once we have identified and resolved any conflicts, we could ensemble independent estimates of $R_t$ from these models into a combined estimate, or fit a joint model with a shared $R_t$ state. If fitting a joint model, only one of the parameters $p_c$, $p_d$ and $c$ needs to be specified, while the other two are in principle identifiable from the data. If the steps above revealed some likely time-dependence in these quantities, this could prompt us to refine the model, for example by allowing the case ascertainment ratio $p_c$ or the shedding rate $c$ to be time-dependent latent states as opposed to fixed parameters \citep{watson2024jointly}. 
\end{enumerate}

\subsection{Case Study 4: Individual-Level Data (Cases and Transmission Pairs)}

In this final case study, we illustrate how we can combine different types of data (time series count data and individual-level data from contact tracing records) to enable estimation of additional quantities. 

\begin{enumerate}
   \item \textbf{Research question and target estimands:} The research question for this case study is what is the time-varying reproduction number $R_t$ and how much heterogeneity is there in individual transmission rates? The latter is captured by the additional estimand $k$, representing the dispersion in the distribution of the number of secondary cases per index case \citep{lloyd2005superspreading}. 
   
    \item \textbf{Process DAG development:} Now that we have some information on individual-level variables, as opposed to just aggregate daily counts, we can return to our original process model (Section~\ref{sec:process}) for the full epidemic transmission tree from case study 1, defined by Eq. \eqref{eq:individual_level} (see Figure \ref{fig:case_study_diagram}). 
    Building on the model of \citet{lloyd2005superspreading}, if $Y\sim \Gamma(1,k)$ for some dispersion parameter $k$, then the number of secondary cases caused by an individual who was infected at a given time $T_i=t$ is
     \begin{equation} \label{eq:offspring_dist}
        Z_i \ | \ (T_i=t) \sim \mathrm{NegBin}\left( \tilde{R}_{t}, k\right)
    \end{equation}   
    where $\tilde{R}_t= \sum_s R_{t+s} w_s$ is the effective reproduction number averaged over the infectious period of an individual who was infected on day $t$.
   
    \item \textbf{Data source selection:} Case studies 1--3 only consider aggregate-level data on daily counts or wastewater measurements. This type of data typically only allows estimation of average transmission, i.e. $R_t$. In contrast, contact tracing records identify transmission pairs, which contain information about heterogeneity in transmission patterns, contingent on contact tracing system quality. The smaller the dispersion parameter $k$, the more variance would be expected in the distribution of the number of secondary infections per index case.
    
    \item \textbf{Observation DAG construction:} Suppose that, for some subset of reported cases $i$, the number of observed secondary cases $Z^\mathrm{obs}_i$ linked to index case $i$ is recorded. For simplicity, we assume that the index cases for whom these data are available are a randomly chosen subset of all reported cases, and each secondary infection independently has a fixed probability $p_l$ of being linked to the index case. We assume that all linked secondary cases are associated with the correct index case (i.e. we ignore any misclassification and uncertainty about who infected whom). 

   Then we can calculate the probability that an index case reported on day $\tau_i$ has $Z^\mathrm{obs}_i$ linked secondary cases by conditioning on the unobserved time of infection $T_i$:
   \begin{equation}
       P(Z^\mathrm{obs}_i=z \ | \ \tau_i=t) = \sum_{s=1}^{t} F_{NB}(z; p_l\tilde{R}_s,k) P(T_i=s | \tau_i=t)
   \end{equation}
where $F_{NB}(.;\mu,k)$ is the probability mass function for a negative binomial distribution with mean $\mu$ and dispersion $k$, and $\tau_i$ is the reporting time for case $i$.  We can calculate the conditional probability on the right-hand side of this equation via Bayes' theorem to give
   \begin{equation} \label{eq:Zobs}
       P(Z^\mathrm{obs}_i=z \ | \ \tau_i=t) = \frac{\sum_{s=1}^{t} F_{NB}(z; p_l\tilde{R}_s,k) \alpha_{t-s} I_s}{\sum_{s=1}^{t}\alpha_{t-s} I_s}  
   \end{equation}
 where $\alpha_t$ is the distribution of time from infection to reporting.

    \item \textbf{Refine the model DAGs:}  We start by assuming that the relative infectiousness profile over time $w_t$ and the distribution of time from infection to reporting $\alpha_t$ are the same for all individuals. This is a reasonable starting assumption, but it may be important to relax this in some situations, for example to model the impact of quarantine and isolation measures on individuals identified by contact tracing.

    \item \textbf{Modularise DAGs:} We modularise the observation DAG into a sub-DAG for daily case incidence $C_t$ and a sub-DAG for observed secondary case counts $Z_i$ for each index case $i$ (see Figure \ref{fig:case_study_diagram}).  
    
    \item \textbf{Inference and implementation choices:} We suggest using NUTS with data augmentation for unobserved transmissions, chosen for efficient handling of discrete latent variables. Alternatively, we could use PMCMC methods, such as particle marginal MetropolisâHastings (PMMH), to estimate the time-varying reproduction number $R_t$ and the fixed parameter $k$ hierarchically, but this may be computationally slow. 

    \item \textbf{Model specification and validation:} Individual-level data enables direct overdispersion estimation without distributional assumptions but requires assumptions about the completeness of contact tracing data. This shows how data granularity can fundamentally change model structure and inference requirements. Note that the observed variable $Z^\mathrm{obs}_i$ in Eq. \eqref{eq:Zobs} may not be available in real time as information about linked secondary cases takes time to collect. Furthermore, the distribution of $Z^\mathrm{obs}_i$ depends on future values of the latent variable $R_t$ and this means the model is not suitable for real-time inference. If the model was found to be impractical for these reasons, it may be necessary to simplify the model by making some additional assumptions. For example, if there was a period of time in which $R_t$ was estimated to be relatively steady, we could simplify the model by assuming a constant reproduction number and estimating $k$ from contact tracing data relating to that period. 

    \item \textbf{Data integration choices:} Since the two datasets in this case study each inform different quantities (case counts inform $R_t$, data on transmission pairs inform $k$), it would not make sense to fit separate models in this example and a joint model is needed. 
\end{enumerate}




\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/case_study_diagram.drawio.pdf}
    \caption{\textbf{A Schematic for Constructing the DAGs.} This schematic shows how data source selection and model choices can lead to different construction of the observation DAG from the underlying process DAG for various case studies.}
    \label{fig:case_study_diagram}
\end{figure}

\section{Discussion}

%\subsection{Summary}

We presented a systematic workflow for infectious disease modelling that addresses domain-specific challenges and extends established Bayesian workflow principles \citep{gelman2020bayesian}, with particular focus on integrating multiple data sources.
This workflow serves both as guidance when developing infectious disease models and as a resource for readers evaluating modelling papers, whether they work in academic research, public health agencies, or policy contexts that rely on model outputs.
Four case studies demonstrate this workflow using progressively more data sources for reproduction number estimation, showing how systematic workflow practices enable principled model development from single-source baselines to complex multi-stream integration.
Each progression shows how additional data sources alter both process and observation DAGs whilst requiring new integration and fitting decisions.
Each case study also leads to new fitting choices and model validation strategies.
Throughout, we highlight that considering each submodel in isolation is a useful starting point for model development and validation.

%\subsection{Strengths and Limitations/Outstanding Challenges}

The systematic structure of our workflow addresses a critical gap in infectious disease modelling practice, where ad hoc approaches often lead to inadequate validation and limited transparency in methodological choices.
A key strength of our framework is the explicit separation of process and observation DAGs, which provides clarity for model development and validation by recognising that epidemiological processes and observation mechanisms draw from different domains.
A significant limitation is that we have not implemented the workflow in practice, providing only schematic case studies that illustrate conceptual progression rather than actual code or examples.
Despite this lack of implementation, the framework's modular structure allows users to adopt specific components like systematic data review or DAG-based planning independently.
They can also use our workflow as a guide, combined with tools of their choice.
Our structured data characterisation checklist provides an important benefit by establishing a common language for discussing data source trade-offs within modelling teams.
Yet this checklist is necessarily context-dependent, with criteria and their relative importance varying across pathogens, settings, and research questions.
This means that we cannot give an overview of all data sources, or even many common ones.
A final strength of our proposed framework is its applicability across diverse contexts, from early outbreak analysis to routine surveillance, offering consistent structure regardless of data availability.

%\subsection{Comparison with Existing Literature}

Our workflow extends Gelman et al.'s general Bayesian workflow framework \citep{gelman2020bayesian} by addressing systematic workflow adoption challenges specific to infectious disease modelling.
Gelman et al. provide comprehensive model validation and criticism within a single modelling cycle with extensive guidance and practical examples.
We embed their modelling loop within a structured workflow addressing domain-specific challenges: data source characterisation, integration strategies, and workflow iteration as outbreaks evolve. 
Whilst we don't give fully implemented examples due to length and time constraints, we do provide additional domain-specific guidance for surveillance data quality, reporting biases, and epidemiological process representation not covered in general statistical workflows.
 
Recent work has made progress towards structured infectious disease modelling workflows, though implementation remains incomplete.
For example, \citet{bouman2024bayesian} systematically compared modelling choices for SARS-CoV-2 transmission and provided an open-source implementation through their HETTMO package.
However, their approach, while methodologically sound, lacks the structured workflow guidance needed for broader application.
Key workflow elements such as prior predictive checks, systematic model criticism, and clear decision frameworks are absent.
Their work highlights the gap between implementing models and following robust and reproducible workflow practices.
In contrast, our framework provides explicit structured guidance through each workflow stage, with case studies demonstrating not just model implementation but the decision process itself.

%\subsection{Future work}

Future progress requires infrastructure that bridges the gap between methodological best practice and implementation.
Whilst this paper provides a schematic workflow, practical tutorials demonstrating actual implementation would make it easier to follow.
However, such tutorials and tools require advances in software composability, where self-contained components can be combined to build complex models.
This would enable domain experts to contribute specialised components without rebuilding core functionality and make all steps of our proposed workflow easier to implement.
Improved tooling for Markov melding would facilitate modular integration, allowing teams to combine submodels without full joint specification or specialist knowledge. Further methodological work on cut posteriors \citep{liu2025general} is needed before easy to use tools to implement them can be developed.
Computational scalability remains an area where more work is needed, particularly for joint analysis of individual-level data with population surveillance.
Recent advances in machine learning-augmented fitting methods show promise, with neural network-based summary statistic construction \citep{raynal2019abc} and deep learning phylodynamic methods \citep{voznica2022deep} providing alternatives to traditional likelihood evaluation.

%\section{Conclusions}

Systematic workflow practices, such as those discussed in this work, benefit infectious disease modelling by providing structured guidance for model development, validation, and multi-source integration that can adapt to evolving outbreak contexts.
Our workflow aims to address the slow adoption of rigorous practices in infectious disease modelling through structured steps from data characterisation to model validation, with multi-source integration as a key challenging application.
The modular approach we advocate enables adding complexity incrementally whilst maintaining transparency about modelling assumptions and integration choices.
We recommend that the infectious disease modelling community adopt a structured workflow, such as the one presented here, for both model development and evaluation. This is particularly important during rapidly evolving outbreaks where research questions shift and new data sources emerge.

\section*{List of Abbreviations}
\begin{table}[h]
\centering
\begin{tabularx}{\textwidth}{lX}
\toprule
\textbf{Acronym} & \textbf{Full Form} \\
\midrule
BFMI & Bayesian Fraction of Missing Information \\
ELBO & Evidence Lower Bound \\
ESS & Effective Sample Size \\
\bottomrule
\end{tabularx}
\end{table}

\section*{List of Abbreviations}
\begin{acronym}
  \acro{BFMI}{Bayesian Fraction of Missing Information}
  \acro{PPL}{probabilistic programming language}
  \acro{MCMC}{Markov Chain Monte Carlo}
  \acro{ELBO}{Evidence lower bound}
  \acro{ESS}{Effective Sample Size}
  \acro{NUTS}{No-U-Turn Sampler}
  \acro{SMC}{Sequential Monte Carlo}
  \acro{PMCMC}{Particle Markov Chain Monte Carlo}
  \acro{ABC}{Approximate Bayesian Computation}
  \acro{HMC}{Hamiltonian Monte Carlo}
\end{acronym}

\section{Acknowledgements}

We thank the organisers and participants of the workshop ``Analysis and modelling for the design of future epidemic surveillance systems'' (CIRM, Marseille, 28 Aprilâ2 May 2025) for valuable discussions and feedback that helped shape this work.
All workshop participants provided useful discussion and feedback.
We thank Poppy the dog for making sure to ask the important questions.
\pagebreak
\bibliographystyle{plainnat}
\bibliography{references,zotero-references}


\end{document}
 