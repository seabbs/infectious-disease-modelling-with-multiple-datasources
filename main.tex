\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{authblk} % For author affiliations
\usepackage{hyperref} % For hyperlinks
\usepackage[margin=1in]{geometry} % Standard margins
\usepackage{natbib} % For bibliography management
\usepackage{enumitem} % For customising lists
\usepackage{color}
\usepackage{amsmath}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow} 
\usepackage{array} 
\usepackage{pdflscape}

\newlist{tree}{itemize}{10}
\setlist[tree]{label=-}
\setlistdepth{10} 
\usepackage{comment}

\title{Workflow Best Practices for Infectious Disease Modelling: Multiple Data Sources, Evolving Outbreaks, and Practical Implementation}

\author[1]{Sam Abbott}
\author[2]{Xiahui Li$^\dagger$}
\author[3]{Punya Alahakoon$^\dagger$}
\author[4]{Dhorasso Junior Temfack Nguefack$^\dagger$}
\author[5]{Johannes Bracher}
\author[6]{Felix Günther}
\author[7]{@working-group-members}
\author[8]{@workshop-participants}
\author[9]{Mircea T. Sofonea}
\author[10]{Michael Plank}
\author[12]{Anne Cori$^*$}
\author[11]{Anne Presanis$^*$}

\affil[1]{London School of Hygiene \& Tropical Medicine}
\affil[2]{University of St Andrews}
\affil[3]{University of Oxford}
\affil[4]{Trinity College Dublin}
\affil[5]{Karlsruhe Institute of Technology}
\affil[6]{Robert Koch Institute}
\affil[7]{@working-group-affiliations}
\affil[8]{@workshop-participant-affiliations}
\affil[9]{University of Montpellier, France}
\affil[10]{University of Canterbury, New Zealand}
\affil[11]{MRC Biostatistics Unit, University of Cambridge}
\affil[12]{Imperial College London}

% Note: Author order is provisional and subject to change

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{$^\dagger$Equal contribution}
\footnotetext[2]{$^*$Joint last authors}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
%Lead: Sam Abbott
Infectious disease modelling increasingly relies on integrating multiple data sources, with recent outbreaks demonstrating how rapidly evolving data landscapes create both opportunities and challenges for modellers seeking to integrate diverse data streams.
These settings are characterised by evolving research questions, emerging data sources, and adapting surveillance systems that require flexible analytical approaches.
The infectious disease modelling community has been slow to adopt rigorous Bayesian workflow approaches for integrating diverse data streams which might help mitigate these domain specific challenges, even as these methods become increasingly formalised and widely used in other domains.
This paper provides a methodological framework for integrating multiple data sources in infectious disease modelling that serves both as guidance for practitioners and as a checklist for readers evaluating modelling papers, building on non-domain-specific Bayesian workflow advice with transmission intensity estimation as a key exemplar.
We first outline an approach for characterising epidemiological data source properties through a structured checklist,  then present an iterative workflow that extends established Bayesian model development approaches to the infectious disease domain and that references the data checklist throughout.
Our workflow progresses from research question definition through development of directed acyclic graph (DAG) representations of process and observation, to integration method selection, with explicit consideration of data conflicts and uncertainty quantification.
We compare integration approaches, from full joint modelling to ensemble methods, and demonstrate through schematic case studies how practitioners can navigate real-world trade-offs between model complexity, computational feasibility, and inferential goal directed acyclic graph directed acyclic graphs.
The case studies show how different data types, including individual-level observations, provide complementary information for estimating parameters such as time-varying reproduction numbers and overdispersion of transmission dynamics.
Throughout, we advocate Bayesian thinking for principled model development, regardless of the ultimate fitting approach.
Our framework emphasises parsimony, interpretability, and model criticism.
We also provide practical guidance supported by schematic worked examples.
\end{abstract}

\section{Introduction}
% Lead: Sam Abbott

% Paragraph 1: Motivation and Context
Infectious disease modelling increasingly relies on integrating multiple data sources to improve parameter estimation, reduce uncertainty, and provide more robust evidence for public health decision making \citep{deangelis2018analysing}.
Recent outbreaks including COVID-19, mpox, and Ebola have highlighted both the potential value and practical challenges of combining diverse data streams such as case reports, deaths, hospitalisations, genomic sequences, wastewater surveillance, and serological surveys \citep{knock2021key, ro2025estimating, Abbott2021-delta, Abbott2022-prevalence, Ward2024-sp}.
These outbreak settings create unique pressures where novel data streams emerge rapidly, surveillance systems evolve to meet changing needs, and models must be developed under severe time constraints with limited understanding of new data sources \citep{mccaw2023role}.
Single data sources often provide limited or biased information about key epidemiological parameters, whilst multiple sources can offer complementary perspectives that improve model accuracy and reliability \citep{lison2024generative, sherratt2021exploring}.
Additionally, in some cases, some parameters are only identifiable by combining data sources that together indirectly inform latent parameters, which are not directly observed.
However, practitioners face complex methodological choices about how to combine these data streams effectively.

% Paragraph 2: Current Approaches
Current approaches to multi-source integration broadly fall into two categories: pipeline methods that fit separate models to individual data sources before combining estimates, and joint modelling approaches that simultaneously fit all data sources within a unified statistical framework \citep{deangelis2018analysing, russell2024combined}.
Pipeline approaches offer computational efficiency and modular development, but may propagate errors and fail to capture dependencies between data sources \citep{lison2024generative, Ward2024-sp}.
Joint modelling can provide more principled uncertainty quantification and better parameter identifiability, but often requires substantial computational resources and model complexity \citep{russell2024combined, lison2024generative}.
Recent empirical evidence demonstrates that joint approaches consistently outperform stepwise methods by avoiding error propagation and improving parameter identifiability \citep{lison2024generative}.
Beyond these choices, practitioners face numerous challenges, including: detecting and resolving conflicts between data sources; combining data sources with different spatial or temporal resolutions; validating models when different data streams suggest different dynamics; and navigating branching decision paths where integration choices impose model structure constraints \citep{deangelis2018analysing, nicholson2022interoperability}.
Fitting challenges, such as computational intractability, parameter non-identifiability, and the need to approximate ideal model structures for practical inference, further complicate implementation \citep{corbella2022inferring, Ward2024-sp}.
These integration and fitting considerations can impact model design, yet their implications are rarely made explicit in published analyses.
Many complex multi-source models lack transparent reporting of workflow decisions, validation procedures, and code availability, creating barriers to evaluation and reproduction [@Ward2024-sp; @Fyles2024-qz; @Abbott2021-delta; @Abbott2022-prevalence].
The infectious disease modelling community has not widely adopted rigorous Bayesian workflow practices that emphasise iterative model development, systematic model criticism, and principled uncertainty quantification \citep{gelman2020bayesian, nicholson2022interoperability}.
Existing guidance for practitioners is fragmented across methodological literature, with limited practical frameworks for navigating these interconnected choices systematically \citep{nicholson2022interoperability, deangelis2018analysing}.

% Paragraph 3: Paper scope and contribution
This paper provides a framework for integrating multiple data sources in infectious disease modelling that serves both as guidance for practitioners and as a checklist for readers evaluating modelling papers, with practical implementation as the primary focus. 
Our approach builds on established Bayesian workflows [@gelman2020bayesian], encouraging iterative model building with principled uncertainty quantification. We advocate for Bayesian thinking throughout the model development process, including model specification, model criticism, and predictive checks, regardless of whether the final implementation uses Bayesian or frequentist fitting methods. 
We first introduce a checklist-based approach for reviewing data source characteristics. 
We then present an iterative workflow for model development that progresses from research question definition through development of directed acyclic graphs (DAGs) representing process and observation models, data source integration, fitting choices, and model validation. 
This workflow references the data characterisation checklist at key decision points. We highlight the feedback loops between steps where choices at later stages require revisiting earlier decisions. 
We summarise how decisions may need to change as outbreak contexts evolve, use cases shift, or new data sources become available. 
We provide an overview of approaches for each step as well as practical guidelines for integration choices, validation strategies, and conflict resolution between data sources. 
Finally, we use four case studies centred on reproduction number estimation to demonstrate this workflow, progressing in complexity from a single-source baseline, through integrating similar data sources, to more complex examples with data of different kinds and scales. 
These case studies demonstrate broader principles applicable across infectious disease modelling contexts. 
By focusing on this common estimation task, we provide practitioners with a concrete foundation for adopting rigorous workflow practices that can then be extended to more complex modelling challenges.

\section{Data Sources and Characteristics} \label{sec:datareview}
% Lead: Punya Alahakoon

% Why do we need this
\paragraph{}There is an increasing amount of data sources that are being used for infectious disease surveillance and modelling, including case notifications, hospitalisations, viral load measurements from clinical samples, and wastewater samples. While each of these data sources offers unique advantages, such as timelines, population coverage, and XXX, they also come with distinct limitations, including reporting delays,  measurement noise, or under-representativeness. Despite their growing use, there is currently no standardised framework to systematically assess and compare these data sources in terms of their strengths and weaknesses. This lack of standardisation hinders our ability to compare, integrate, and prioritise data streams effectively across contexts. 
% Data  sources have characteristics, we can record these, key characteristics. Full list in SI
\paragraph{}Different data streams posses a range of intrinsic characteristics. While these characteristics vary across different data sources, we think that some characteristics are consistently important for evaluating their strengths and weaknesses(Figure \ref{data_characteristics}).  These charateristics are (1) \textit{metadata}  that encapsulates the foundational information about each data stream; (2) \textit{scope} of a data source that refers to the representative breadth and epidemiological relevance of a data source; (3) \textit{Resolution} that describes the granularity and the level of details captured by a data source; (4) \textit{Data quality} that encompasses the reliability, accuracy and completeness of the measurements within a data stream; (5) \textit{Data utility} that refers to the practical applicability of a data source in informing epidemiological metrics and supporting public-health decision making; (6)\textit{ Practical considerations} that addresses the feasibility of using a data stream i n modelling and surveillance contexts (see Supplementary Material for more details). 
% From these characterise we can construct a check list to assess data. Good times
\paragraph{}We propose that these six characteristics be used as a checklist to guide the systematic assessment of a data stream's strengths and weaknesses. While the precise structure, relevance, and sub-attributes of each component may vary depending on the specific surveillance and modelling objectives, and the nature of the data, the key objective of this checklist is to promote structured, transparent thinking about the data at hand. Rather than relying on ad-hoc or judgments, this checklist encourages users(??) to explicitly consider critical dimensions when evaluating various data sources. This approach may support more rigorous data selection, integration, and interpretation in infectious disease modelling. 


\begin{figure}[H] 
\includegraphics[width=1\linewidth]{figures/data_characteristics_2.png}
\centering
\caption{The proposed checklist for characterising a data stream used in infectious disease surveillance and modelling. The six core characteristics---metadata, scope, resolution, data quality, data utility and practical considerations---are illustrated alongside representative attributes that can be used to assess the strengths and limitations of each data source. }
\label{data_characteristics}
\end{figure}


\section{Workflow}
% Lead: Sam Abbott

\subsection{Overview}



\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/visualization of core steps.drawio.png}
    \caption{Recommended workflow for integrating multiple data sources in infectious disease modelling. Begin by defining research questions and target estimands, proceed through iterative development of process and observation DAGs, and follow critical decisions about data integration and inference methods. Key feedback loops from downstream parts of the workflow that impact earlier choices are represented with dashed arrows and boxes.}
    \label{fig:workflow}
\end{figure}

We recommend following a structured, iterative workflow for multi-data source modelling (Figure~\ref{fig:workflow}). Our suggested workflow extends established Bayesian workflow principles [@gelman2020bayesian] for  the specific challenges of multi-source infectious disease modelling.
We advocate this approach because it makes modelling choices transparent, assumptions explicit, and provides principled tools for assessing model adequacy regardless of whether final inference is Bayesian or frequentist.

Begin with \textbf{decision making}: clearly define your research question and target estimands (e.g., time-varying reproduction number, overdispersion parameters).
Next, we rely on the commonly used \textbf{directed acyclic graphs (DAG)} approach to define our models. However, we depart from standard approaches in that we suggest decoupling the definition of the \textbf{process} and \textbf{observation} models.
Here the \textbf{process model (DAG)} represents the underlying epidemiological process. 
A key part of this workflow is iterating on this representation as understanding develops i.e. during an outbreak.
The next step is to make use of the data source characterisation from the last section to \textbf{map available \textbf{data sources} }to your process model, such as incidence time series, genomic data, contact tracing, viral load measurements, and serological surveys based on those identified in the previous section.
Data sources should be selected based on their expected contributions to the research question.
For each data source, develop an \textbf{observation DAG} linking the underlying process to observed data through measurement models and reporting mechanisms.
Different data sources may also impact your \textbf{process DAG} assumptions, such as if you can collapse your representation of the process from individual-level to population-level.

Once you have developed your \textbf{process} and \textbf{observation DAGs}, proceed to \textbf{model specification and validation}, including prior specification, parameter identifiability assessment, and prior predictive checks.
You then face the key decisions \textbf{data integration choices}: \textbf{``What to combine?''} and \textbf{``How to combine?''}.

If combining multiple sources is not beneficial or feasible, you can proceed to single-source modelling and potentially the combination of estimates e.g. where multiple models estimate the effective reproduction number from different data sources and so these estimates can be ensembled [@citationspim].
If integration is warranted, select among \textbf{data integration} methods. 
Once your data sources have been integrated, it is time to make \textbf{fitting choices} i.e. selecting an inference method based on the model structure, integration approach, and theoretical and practical considerations. 
Finally, \textbf{fit your model} and evaluate its performance through posterior predictive validation and sensitivity analysis.

Throughout this workflow, there is the potential for feedback loops between steps so that model development is rarely a single forward pass (see Figure~\ref{fig:workflow}). 
Data characteristics alter process DAG structure; individual-level data requires different representations than population-level data. 
Identifiability issues discovered during model specification may require simplifying either the process model or observation models. 
Practical constraints shape integration choices in multiple ways. Teams using incompatible programming languages may necessitate modular rather than joint approaches. 
Integration complexity itself may require simplifying observation models to maintain computational feasibility. Conflicts between data sources during integration often reveal model misspecification, requiring revision of earlier assumptions. 
Computational constraints during fitting may require approximating or restructuring the process DAG. Limited computational resources or available expertise may favour modular over joint integration approaches despite the theoretical advantages of joint modelling. 
For example, if you initially design a joint model but find it computationally intractable with available methods, you might iterate back to choose a modular integration approach, which in turn might influence how you structure your observation DAGs.

Practitioners should expect to repeat this workflow as outbreaks evolve.
Understanding of pathogen characteristics improves over time, research questions shift from immediate response to longer-term planning, and decision-making timescales change from days to months.
New data sources emerge whilst surveillance systems adapt to changing circumstances.
This evolution favours modular approaches that enable component reuse rather than bespoke models requiring complete redevelopment when applications change.

In the following sections, we will discuss each of these steps in more detail as well as providing resources for learning more. We will also demonstrate this workflow in our case studies.
Critical to this workflow is transparent reporting of all decisions and validation procedures, enabling both reproduction and adaptation of models for future applications.

\subsection{Research Question and Target Estimands}

Determining the research question shapes the entire modelling workflow, from data source selection through integration choices to validation strategies.
Start by engaging with policymakers and/or stakeholders to understand which epidemiological parameters matter for their decisions: some need simple short-term case forecasts for hospital planning, others require transmission dynamics understanding for intervention design \citep{marshall2024when}.
Often, multiple outcomes are desired that require balancing trade-offs: short-term forecasts for operational planning alongside $R_t$ estimates for public communication and interpretability.
The aim is to translate policy needs into specific target estimands by trying to understand how parameters inform decisions \citep{nicholson2022interoperability}.
For example, policymakers might need to know if transmission is increasing rather than precise values, or whether superspreading drives transmission, to choose between population-wide versus targeted interventions.
Try to define scope explicitly, including what the analysis cannot address: a national $R_t$ estimate cannot easily reveal local outbreak dynamics, aggregate case data might not be able to identify transmission chains, and surveillance might not be able to detect mild infections.
These limitations shape stakeholder expectations and prevent misuse of results.
Research questions evolve throughout outbreaks as understanding improves and policy needs shift.
Early outbreak questions focus on growth rates and severity using limited case data.
As surveillance expands, questions might shift, for example, to variant dynamics requiring genomic integration, then to population immunity needing serological data.

\subsection{Process DAG Development}

Process DAG development translates research questions into formal causal structures that evolve throughout an outbreak.
A directed acyclic graph (DAG) represents causal relationships between variables through nodes (variables) and directed edges (causal influences), making assumptions explicit and testable \citep{digitale2022tutorial}.
This formalism helps identify confounders, mediators, and colliders that affect parameter estimation from observational data.
For infectious disease modelling, we recommend constructing DAGs that capture biological mechanisms: susceptible populations, transmission events, infection progression, recovery or death, and intervention effects.
Begin with simple representations of core transmission dynamics, then iteratively refine as understanding improves during an outbreak.
Early outbreak DAGs might represent homogeneous mixing, while later versions incorporate age structure, spatial heterogeneity, or variant dynamics as data reveals their importance.
Key biological mechanisms drive DAG structure: incubation periods create delays between infection and infectiousness, viral load dynamics affect transmission probability, and immune waning influences reinfection risk.
We suggest separating process DAGs from observation DAGs to distinguish true epidemiological processes from how we measure them.
This separation enables modular model development where process components can be reused across different surveillance contexts \citep{nicholson2022interoperability}.

\subsection{Data Source Mapping}

Data source mapping connects available surveillance streams to epidemiological processes using the characteristics framework from Section~\ref{sec:datareview}.
Catalogue available data systematically using the six-dimensional checklist: metadata, scope, resolution, data quality, data utility, and practical considerations.
This structured assessment reveals each source's strengths and limitations for addressing target estimands.
Link data sources to process components by identifying what epidemiological quantities each stream measures: cases reflect infection incidence with reporting delays and ascertainment bias, hospitalisations capture severe outcomes with admission thresholds, and wastewater indicates population-level viral circulation independent of healthcare seeking.
Assess data characteristics and biases explicitly and document collection processes that might create selection bias or differential participation \citep{digitale2022tutorial}.
Evaluate complementarity by examining whether sources provide independent signals or reinforce the same information.
For example, wastewater can potentially early warning, while clinical surveillance provides individual-level severity data.
Identify redundancy where multiple sources inform the same parameter, which might improve precision at the cost of model complexity.
Start with minimum data requirements for target estimands, then expand systematically.
Each additional source increases complexity and development time, so prioritise based on expected information gain relative to implementation cost.
Document these mapping decisions explicitly as they shape all downstream workflow choices from observation DAG construction through integration methods.

\subsection{Iterate on the Process DAG}

After mapping data sources, revisit your process DAG to ensure alignment between what you want to model and what your data can support \citep{gelman2020bayesian}.
Data availability can drive both increases and decreases in model complexity.
Contact tracing data with identified transmission pairs enables shifting from population-level compartmental models to individual-based representations that capture heterogeneous mixing patterns.
Conversely, discovering that surveillance only provides aggregate counts may require collapsing a planned age-structured model into simpler compartments.
Strong prior information can sometimes compensate for data limitations, functioning almost like an additional data source.
Well-informed priors about transmission parameters from previous outbreaks might support maintaining a complex structure even when current data is limited.
For example, detailed knowledge about age-specific contact patterns from prior studies could support maintaining the age structure, even with only total case counts.
Note that assuming fixed parameter values, common in mechanistic modelling literature, is effectively placing infinitely strong priors and should be avoided when possible in favour of appropriately uncertain prior distributions.
Temporal resolution affects what variation can be identified rather than what can be modelled: you can always build a daily-scale model, but weekly surveillance data will usually inform weekly or longer-term patterns, not daily fluctuations.
Additional process DAG iteration typically occurs at key workflow stages, such as during model specification when identifiability issues emerge, during validation when misalignment between model structure and data patterns is exposed, during data integration, or due to practical or theoretical considerations related to model fitting (Figure~\ref{fig:workflow}) \citep{corbella2022inferring}.
Each iteration should address specific identified problems rather than arbitrary complexity reduction.
Document why changes were made, preserving the rationale for future model extensions when better data becomes available or computational methods improve.

\subsection{Observation DAG Construction}

Observation DAGs map how latent epidemiological processes generate observed data, with complexity shaped by target estimands and available data sources \citep{deangelis2018analysing}.
Build these DAGs by working backwards from measured values through all intermediate steps: symptom onset, healthcare seeking, testing, laboratory processing, and reporting for clinical surveillance; viral shedding, sewage networks, and quantification for wastewater.
Each step introduces delays, biases, or missing data mechanisms that affect inference.
The level of detail depends on research questions and later workflow choices.
Estimating population-level transmission requires simpler observation models than understanding variant-specific dynamics or age-stratified patterns.
Process DAGs may need revision based on observation DAG constraints: individual-level data enables different process representations than aggregated reports.
Model reporting delays explicitly as they create temporal misalignment between transmission events and observations.
Distinguish between missing data mechanisms since random missingness and systematic under-ascertainment require different handling \citep{sherratt2021exploring}.
Account for time-varying observation probabilities from testing capacity or surveillance intensity changes.
Account for hierarchical structure and population heterogeneity when multiple data sources capture different demographic subgroups.
Multiple observation DAGs for the same underlying process enable validation by checking whether different data streams imply consistent transmission dynamics when properly aligned.
This separation of process and observation enables modular development where components can be reused across surveillance contexts.


\subsection{Model Specification and Validation}\label{sec:spec-validate}

Start by validating models in small subcomponents first, then explore combinations to identify integration challenges early.
Specify priors using domain knowledge where available through informative priors, otherwise use weakly informative priors that regularise without dominating the likelihood \citep{gelman2020bayesian}.
Validate using prior predictive checks, which simulate data from the prior to ensure model behaviour aligns with epidemiological plausibility before seeing data.
Assess parameter identifiability by examining whether data sources provide sufficient information to uniquely estimate target parameters; joint modelling often resolves identifiability issues that plague pipeline approaches \\citep{lison2024generative, russell2024combined}.
Detect conflict between data sources, where different streams imply incompatible parameter values under the current model.
When conflict arises, use prior sensitivity analysis to quantify how much priors must change to accommodate observed data, revealing potential model misspecification or data quality issues \citep{roos2015sensitivity,nott2021approximation,kallioinen2021detecting}.
This conflict analysis connects directly to the value of information assessments that prioritise data collection efforts \citep{jackson2019value}.
Criticise fitted models through posterior predictive validation, simulating new data to test whether models reproduce key features across all integrated sources.
Apply cross-validation strategies, particularly leave-one-out and time-aware validation, to assess predictive performance while respecting temporal data structure.
Conduct sensitivity analyses by examining how conclusions change across prior specifications, likelihood assumptions, and data inclusion decisions.
Ideally, use simulation-based calibration for each submodel to verify that inference algorithms can recover true parameters from simulated data, as this provides stronger validation than posterior predictive checks alone \citep{talts2018validating}.
For comprehensive implementation guidance, see \citet{gelman2020bayesian}, with infectious disease-specific adaptations in \citet{bouman2024bayesian}.
Model specification and validation have a critical feedback loop with DAG structure and data integration choices.

\subsection{Data Integration Choices}
% Lead: Anne Presanis

%Consideration: How do we clearly split here between combined and individual approaches? What does that structure look like? How do we connect this question to fitting choices which in practice are important?
%Consideration: What parts of this (sub models etc etc are part of specification and validation. How does that interaction work?
%Consideration: There might be an ideal DAG structure but in practice need to approximate for fitting. I.E want to use NUTs so can't have latent discrete parameters etc...

\subsubsection{Decision Framework}
% When to integrate vs when to keep separate
% Assessing complementarity and redundancy
% Computational vs inferential trade-offs

In principle, integrating multiple data should enhance the identifiability and precision of parameter estimates \citep{deangelis2018analysing, lison2024generative, russell2024combined}. Even when data streams are redundant (informing the same quantity), their combination can improve precision, consistent with meta-analytic principles \citep{deangelis2018analysing}. 

In practice, however, data integration may pose challenges. These include inconsistencies due to unaccounted biases \citep{knock2021key, corbella2022inferring, Ward2024-sp}; computational complexity \citep{corbella2022inferring}; and real-time pressures during an outbreak emergency \citep{mccaw2023role}.

Evaluating the strengths and limitations of different data sources, as described in Section \ref{sec:datareview}, can and should be done during inter-epidemic periods. This evaluation, alongside with the workflow of specifying process and observation DAGs (Section ??), helps assess whether estimands of interest are identifiable from the available data. Value-of-information methods \citep{jackson2019value} can formally guide whether new or additional data would meaningfully improve precision. However, for complex models especially, identifiability may only become evident after fitting an initial version.

Decisions about data integration are inherently context-dependent, but can be guided by the decision tree in Figure \ref{fig:integration}. When combining independant model outputs from different groups, the choice of ensembling or meta-analytic approach should align with the research objective (e.g. estimation or prediction), influencing how estimates or predictions are weighted (e.g. by precision or prediction accuracy). In contrast, when integrating sub-models, the approach will depend on whether existing sub-models have already been fitted, or if the models are being developed de novo.

\subsubsection{Integration Methods}

% TODO: Full joint modelling
Specifying a full joint model of multiple data sources and the underlying processes generating them offers a coherent framework for integrating such sources simultaneously, in the spirit of Bayesian hierarchical models \citep{gelman2020bayesian}. This approach is especially suited when there are complex dependencies between data sources, perhaps conditional on the parameters of the underlying process model, such that identification of all parameters is compromised when some data are left out. However, joint models can be computationally intensive and challenging to fit, especially in high-dimensional or real-time settings. Care is needed to ensure complex dependencies are appropriately modelled and that observational biases, such as selection biases, are adequately accounted for in the model, to avoid conflict between data sources (REF). 

% Modular approaches and staged fitting
% Markov melding
A modular model building approach \citep{nicholson2022interoperability} is therefore generally recommended, guided by the principle of parsimony (``Occam's razor''). 
Starting with simple sub-models makes it easier to diagnose issues such as poor fit, model misspecification or convergence issues. Complexity can then be added incrementally, only as far as needed. Sequentially combining sub-models also facilitates the detection of inconsistencies or conflicts between them (REFs). Modular approaches also offer computational advantages over joint modelling. Techniques like Markov melding (REF) allow posterior samples from one sub-model to serve as a proposal distribution for the next, avoiding the need to refit a full joint model. Similar sequential strategies, using the posterior from one sub-model as the prior (rather than proposal) for another, have a long history (REF?).

While the theory and some applications of Markov melding are well-established (REFs), practical implementation remains challenging. Limitations include the availability of user-friendly Bayesian software and methods for assessing sub-model compatibility (REF). As a result, approximate methods are often used. A common approach is the one used in standard meta-analysis (REF) to combine previous estimates using a normal approximation. A Stage 1 point estimate, $\hat{y}_1$, and its corresponding standard error or posterior standard deviation $\hat{\sigma}^2_1$, can be incorporated in a Stage 2 model as a likelihood term, on an appropriate scale:
$$
\hat{y}_1 \sim N(\theta, \hat{\sigma}^2_1)
$$ for some parameter $\theta$ in the Stage 2 model informed by the Stage 1 estimate. This approach has been shown to be an approximation to Markov melding with ``product of experts'' pooling (REF).

% Cut distribution and generalised evidence synthesis
Modular approaches also support selective trust in sub-models. ``Cutting'' feedback -- i.e. preventing less reliable sub-models from influencing more trusted ones while allowing the reverse -- enables nuanced evidence synthesis (REFs).

% TODO: Full joint modelling
% suggested text from copilot in case useful:
% Full joint modelling offers a coherent framework for integrating multiple data sources and processes simultaneously. By specifying a single joint likelihood and prior structure, this approach can capture dependencies and interactions across sub-models more naturally than modular alternatives. It is particularly valuable when feedback between components is essential or when sub-models are tightly coupled.
% However, joint models can be computationally intensive and challenging to fit, especially in high-dimensional or real-time settings. They also require careful specification to avoid identifiability issues and to ensure that all components are appropriately informed by the data. Despite these challenges, joint modelling remains the gold standard when feasible, offering the most principled basis for inference.

% TODO: Ensemble and weighting methods
% suggested text from copilot in case useful:
When multiple models or estimates are available—whether from different research groups or alternative specifications—ensemble methods provide a flexible way to combine them. The choice of ensemble strategy should reflect the research objective. For prediction, weighting by out-of-sample accuracy may be appropriate; for estimation, weights might reflect precision or model credibility (REF). Common approaches include Bayesian model averaging, stacking, and meta-analytic pooling (REF). These methods can accommodate model uncertainty and allow for robustness across competing hypotheses. In practice, ensemble weights may be fixed, data-driven, or informed by expert judgment, depending on the context and available information (REF).


\subsubsection{Conflict Detection and Resolution}
% Prior-data conflict
% Data-data conflict
% Model criticism in multi-source settings
% Sensitivity analysis approaches

Detecting and resolving conflicting evidence about underlying latent processes is essential to ensure coherent inference and robustly informed decisions \citep{sherratt2021exploring}. Such conflicts may arise between prior assumptions and data (``prior-data conflict''), between different data sources, between different partitions of a joint model into submodels, or even within a data source but between different units of data (REF).

If data sources and/or prior information provide inconsistent evidence on a parameter, this conflict usually manifests either as a lack of identifiability, a symptom of which may be lack of convergence of an MCMC algorithm or equivalent; or as lack of fit to one or more data sources (REF). Resulting estimates may therefore be more uncertain than if each data source were modelled independently. Such conflicts are typically the result of model misspecification: some observational bias that has not been accounted for in the observation model (e.g. under-reporting or selection bias); or some approximation in the process model that fails to explain the variation in the data (e.g. a fixed under-reporting proportion); or an unrealistic prior specification (e.g. an infection-fatality risk based on a previous pathogen which is not that similar to a newly emerging one).  

Diagnostic tools to detect conflicts in evidence synthesis models include prior-, mixed- and posterior-predictive checks (REF) and extensions of these to latent levels of a hierarchical model (REF).


% Copilot suggestion to flesh out the above in case useful:
%In multi-source modelling, detecting and resolving conflicts is essential to ensure coherent inference and robust decision-making. Conflicts can arise at various levels:
%Prior–Data Conflict: When prior beliefs are strongly contradicted by observed data, it may indicate misspecification or overly informative priors. Diagnostic tools such as prior predictive checks and posterior–prior overlap metrics can help identify such conflicts early in the modelling process.
%Data–Data Conflict: Different data sources may provide inconsistent evidence about the same underlying process, due to biases, measurement error, or contextual differences. Identifying these conflicts requires careful comparison of marginal likelihoods, posterior distributions, and model fit across data streams.
%Model Criticism in Multi-Source Settings: Traditional model criticism techniques—such as posterior predictive checks, residual analysis, and information criteria—must be adapted to account for the complexity introduced by multiple data sources. Modular approaches can help isolate problematic components, while joint models may require more sophisticated diagnostics.
%Sensitivity Analysis: Sensitivity analysis plays a key role in conflict resolution. Varying priors, data inclusion, or model structure can reveal the robustness of conclusions and help identify influential assumptions. In modular settings, sensitivity analysis can be performed component-wise, while in joint models, global sensitivity approaches may be more appropriate.
%Ultimately, conflict detection should be an iterative process, integrated into model development and refinement. Transparent reporting of conflicts and their resolution enhances the credibility and interpretability of modelling outputs, particularly in high-stakes settings such as outbreak response.


\subsubsection{Practical Constraints}
% Different teams with different expertise/languages
% Software compatibility issues
% Computational resource limitations
% Time constraints in outbreak settings

% Copilot suggestion in case useful to flesh out the above:
% Despite the theoretical advantages of data integration and modular modelling, several practical constraints often limit their implementation in real-world settings:
%Diverse Teams and Disciplinary Silos: Collaborative modelling efforts frequently involve teams with varied disciplinary backgrounds, methodological preferences, and even programming languages. This diversity, while enriching, can complicate communication, model interoperability, and consensus on modelling frameworks.
%Software Compatibility: Differences in software ecosystems—such as R, Python, Stan, or proprietary tools—can hinder seamless integration of sub-models or data pipelines. Even when using the same language, variations in package versions or coding conventions can introduce friction.
%Computational Resources: High-dimensional models or those requiring extensive simulation (e.g., via MCMC) can be computationally demanding. Limited access to high-performance computing infrastructure may constrain the complexity or scale of models that can be feasibly implemented, particularly in low-resource settings.
%Time Constraints in Outbreak Settings: During public health emergencies, the need for rapid decision-making often precludes the development of fully integrated or optimised models. In such contexts, simpler or pre-existing models may be prioritised over more comprehensive but time-intensive approaches.
%These constraints highlight the importance of planning and infrastructure development during inter-epidemic periods, including the creation of interoperable modelling frameworks, shared codebases, and cross-disciplinary training.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/integration choices decision tree.drawio (1).png}
    \caption{Decision tree of integration choices for integrating multiple data sources in infectious disease modelling.}
    \label{fig:integration}
\end{figure}

\begin{comment}
First stab outline 27/05
\begin{itemize}
    \item Different possible choices for integrating/ensembling inference from multiple data sources
    \begin{itemize}
        \item Full joint model fitted, regardless of whether you have already fitted separate sub-models
        \item Conditionally independent sub-models each fitted separately, then integrated
        \begin{itemize}
            \item Meta-analysis of separate estimates
            \item Weighted averaging / ensembling
            \item Markov melding
        \end{itemize}
    \end{itemize}
    \item Depends on whether you are starting from scratch, from an existing model for one data source to which you want to add others, or whether you have multiple alternative sets of existing inferences from different data sources that you want to combine
    \item General principle that modular model building (De Angelis et al, 2015; Birrell et al, 2018; Goudie et al, 2019; De Angelis \& Presanis, 2019; Gelman et al, 2020; Nicholson et al, 2022, Liu \& Goudie, 2025) is preferable, since:
    \begin{itemize}
        \item Easier to understand lack of fit, model misspecification or convergence issues from simpler sub-models individually
        \item Occam's razor - principle of parsimony, start from simplest model and build complexity up only as far as needed
        \item Adding sub-models in one at a time allows for assessment of consistency/conflict between sub-models sequentially
        \item Computational efficiency - rather than fitting full joint models after fitting the sub-models, use the posterior samples from the sub-models to obtain your full joint model (melding or ?)
    \end{itemize}
    \item Choice of likelihood function (or other objective function) and fitting choices (\ref{sec:fitting}) therefore depends on options above on where you are starting from (existing models/sub-models or from scratch)
    \item And model development is a cycle of model building and model criticism
    \begin{itemize}
        \item extension of model validation in \ref{sec:spec-validate} to multiple data sources setting
        \item detection/measurement of conflict not only between prior and data, but between data and data, or partitions of the DAG (modules) comprising different combinations of prior and data, i.e. posterior-posterior comparisons
        %\item TODO: conflict references from Fuming's literature review
    \end{itemize}
\end{itemize}

% TODO: Add practical examples of each approach
% TODO: Include decision framework for choosing integration method
\end{comment}

\subsection{Fitting Choices}\label{sec:fitting}
% Lead: Anne Presanis, with Dhorasso and Xiahui

Inference for infectious disease models typically involves estimating a set of parameters $\theta$ from observed data $Y$, which may include case counts, hospitalizations, deaths, wastewater surveillance or other epidemiological measurements. These data are assumed to arise from a probabilistic model characterized by a likelihood function $p(Y | \theta)$ that links the parameters to the data through both an underlying transmission dynamics (represented by the process DAG in our workflow) and a noisy observation mechanism (captured by the observation DAG).  The choice of inferential approach depends on the tractability of the likelihood: 
whether $ p(Y | \theta)$ is analytically or numerically computable, intractable and requiring approximation, or entirely  unavailable (necessitating likelihood-free, simulation-based methods). This section provides a brief overview of common computational strategies, with a focus on practical considerations and current best practices.. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/Subpanel_fitting choices_v2.drawio.png}
    \caption{Fitting choices workflow for integrating multiple data sources in infectious disease modelling.}
    \label{fig:fitting}
\end{figure}

% \subsubsection{Tools for Tractable Likelihood Functions}
% % TODO: MCMC and its variants for joint models
% % TODO: Sequential Monte Carlo (SMC) approaches

% \subsubsection{Tools for Partially Tractable Likelihood Functions}
% % TODO: Particle MCMC (pMCMC) for state-space formulations
% % TODO: Variational inference (VI) and INLA for specific model classes

% \subsubsection{Tools for Intractable Likelihood Functions}
% % TODO: Approximate Bayesian Computation (ABC-MCMC, ABC-SMC)
% % TODO: ABC with history matching
% % TODO: Bayesian Synthetic Likelihood (BSL)
% % TODO: Simulation-based inference approaches



\subsubsection{Likelihood-based methods}
Likelihood-based inference encompasses frequentist and Bayesian appraoches. Classical frequentist methods such as maximum likelihood estimation (MLE)\citep{myung2003tutorial, baltazar2024maximum} and profile likelihood \citep{tonsing2018profile, plank2024structured} provide well-established tools for parameters estimation. However, here we focus mainly on Bayesian methods due to their flexibility in handling uncertainty and incorporating various sources of information.

Bayesian inference updates prior beliefs $ p(\theta)$ in light of observed data to obtain a posterior distribution, $p(\theta | Y) \propto p(Y|\theta) \,p(\theta)$. For simple models with exponential family likelihoods, conjugate priors that allow closed-form posterior solutions \citep{gelman1995bayesian,  cori2013new}. However, many infectious disease models typically involve complexities that preclude exact inference.

When the likelihood can be evaluated (up to a constant) but closed-form posteriors are unavailable, sampling-based methods, especially Markov Chain Monte Carlo (MCMC) remain the gold standard \citep{gilks1995markov, lekone2006statistical}. Common algorithms includeMetropolis–Hastings \citep{hastings1970monte} and Gibbs sampling \citep{geman1984stochastic}. For latent variable models, data-augmented MCMC introduces auxiliary states to jointly infer parameters and hidden variables \citep{o1999bayesian}. More efficient techniques such as Hamiltonian Monte Carlo (HMC) for differentiable likelihoods \citep{duane1987hybrid} and its adaptive variant No-U-Turn Sampler (NUTS) \citep{hoffman2014no, andrade2020evaluation}, leverage gradient information for faster convergence and automatic tuning. 

Alternatives to MCMC include Variational Inference (VI), which approximates the posterior by optimizing a tractable surrogate distribution, offering scalability at the cost of some bias \citep{blei2017variational, chatzilena2019contemporary}. Similarly, the Integrated Nested Laplace Approximation (INLA) 
offers a computationally efficient method for approximating posterior marginals in latent Gaussian models, without relying on sampling-based methods \citep{rue2017bayesian}.

In state-space models with high-dimensional latent state structures, the likelihood is often intractable due to the need to marginalise over latent states. Sequential Monte Carlo (SMC) methods, or particle filters, approximate the posterior using weighted particles and yield unbiased likelihood estimates \citep{doucet2001introduction}. These can be embedded within MCMC to form particle MCMC (PMCMC) \citep{andrieu2010particle, endo2019introduction} enabling joint parameter and latent state inference. Sequential Monte Carlo Squared (SMC$^2$) \citep{chopin2013smc2} extends this to fully sequential Bayesian updating as new data are accumulated.

\subsubsection{Likelihood-Free Methods}

Likelihood-based methods are powerful for parameter inference butrely on evaluating the likelihood function, which is sometimes infeasible for complex infectious disease models. Likelihood-free methods circumvent this by simulating from the model instead.

Approximate Bayesian Computation is a popular likelihood-free method that approximates the posterior distribution by accepting parameter values that generate simulated data close to the observed data under a predefined distance metric \citep{rubin1984bayesianly, tavare1997inferring, beaumont2002approximate}. While conceptually simple, the classical ABC rejection sampling is inefficient in high-dimensional settings, prompting the development of mode advanced variants. 

ABC-MCMC combines ABC with MCMC to improve sampling efficiency, especially for peaked or correlated posteriors \citep{marjoram2003markov, wegmann2009efficient, kypraios2017tutorial}. However, its performance is sensitive to the choice of tolerance thresholds and proposal distributions.

ABC-SMC further enhances efficiency by iteratively refining  tolerances and proposal distributions, adaptively focusing computational effort on high-probability regions and handling multi-modal posteriors more effectively \citep{sisson2007sequential, toni2009approximate, beaumont2009adaptive, drovandi2011likelihood}. 

When exact likelihoods are unavailable but the model can be simulated repeatedly, synthetic likelihood methods provide an alternative by approximating the likelihood via assuming Gaussian distributions for summary statistics \citep{wood2010statistical, price2018bayesian}. 

\subsubsection{Selecting the Right Tool}
% TODO: Trade-offs between computational complexity, accuracy, bias
% TODO: Interpretability and implementation ease considerations
% TODO: Decision-making frameworks for method selection

Selecting an appropriate inference method involves balancing trade-offs between computational efficiency, statistical rigor, data availability, interpretability, and model complexity \citep{funk2020choices}. Rather than adopting a universal approach, the choice should be tailored to the specific modeling context and inferential goals. 

For evolving epidemics, a key consideration is the dynamic nature of epidemic processes and data streams. As transmission rates and intervention effects change over time, and new data arrive continuously, methods that support sequential data updates, such as SMC, are generally preferred for real-time epidemic tracking \cite{birrell2020efficient, storvik2023sequential}. In contrast, MCMC-based methods, while statistically robust, are typically better suited for retrospective analyses, due to their reliance on complete datasets and high computational demands for reanalysis. 

 When computational resources are constrained, for example during real-time outbreak monitoring or when analyzing large datasets, approximate methods like VI or ABC-SMC might offer scalable alternatives \citep{chatzilena2019contemporary, engebretsen2023real}.

Beyond computational efficiency, statistical accuracy is influenced by algorithmic choices including priors distributions, tuning parameters, tolerance thresholds, and the number of particles or iterations. Full-likelihoods methods typically yield higher accuracy, subject to likelihood tractability and proper specification, whereas likelihood-free approaches often prioritise flexibility and scalability over precision \cite{alahmadi2020comparison}. 

Interpretability and ease of implementation also play a role. Certain methods integrate smoothly with probabilistic programming environments, while others may require further tuning and computational expertise. Ultimately, method selection should followa systematic decision process that considers modeling goals, urgency or latency constraints, system complexity, and available computational resources. 

Regardless of the approach chosen, model checking, sensitivity analysis, and validation are essential to ensure that inferences are not only statistically valid but also scientifically meaningful and actionable.

\subsubsection{Practical Implementation}
% Lead: Sam Abbott
% TODO: Software implementations and availability
% TODO: Diagnostic tools and convergence assessment
% TODO: Computational resource requirements
% TODO: Reference to inference subpanel workflow figure

A wide range of software tools is available to support the implementation of both likelihood-based and likelihood-free methods in infectious disease modeling. Bayesian inference platforms such as Stan, JAGS and NIMBLE are widely used for likelihood-based approaches, offering flexibility in model specification and efficient MCMC algorithms \citep{carpenter2017stan,abril2023pymc}. For more complex or high-dimensional models, Python based PyMC and Julia based Turing.jl provide probabilistic programming environments with good performance \citep{abril2023pymc,ge2018turing,fjelde2025turing}. Using a fully generative probabilistic programming language like Turing.jl or NumPyro makes model validation such as simulation-based calibration easier, as one can easily sample from the model without writing separate model generators required when using other approaches. For more scalable alternatives to MCMC, INLA is implemented in the R-INLA package \citep{martins2013bayesian} and is particularly efficient for latent Gaussian models, including spatiotemporal structures commonly used in disease mapping and surveillance. VI is also available in PyMC and Turing.jl. These implementations allow rapid inference for large datasets or real-time modelling applications.  Additionally, tools such as LibBi support SMC-based inference for state-space models and can scale to high-dimensional problems using GPU and parallel computing \citep{murray2015bayesian}. For likelihood-free methods, ABC is supported in packages like EasyABC, abc, abctools \citep{jabot2013easy,csillery2012abc,nunes2015abctools}.

Computational demands vary substantially between inference methods and can constrain the scale and complexity of implementation. MCMC methods can be computationally intensive for high-dimensional parameter spaces. VI is highly scalable and typically much faster than MCMC. However, it trades accuracy for speed and may not always be suitable where posterior uncertainty is critical. Likelihood-free methods tend to be computationally expensive because of repeated model simulation. Parallelization is essential and HPC resources are often required for large-scale studies. 

% Add some stuff talking about performance on CPU vs GPU etc

\begin{landscape}
\begin{table}[ht]
\renewcommand{\arraystretch}{1.2}
\centering
\caption{\textbf{Comparison of Likelihood-Based and Likelihood-Free Fitting Methods (Could table masters improve the current table format please?).} 
This table focuses on foundational algorithms. More recent methodological advancements that build upon these classical approaches are discussed in Section 4.9 of the main text.}
\label{tab:methods_comparison}
\small
\begin{tabular}{@{}p{3.2cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}@{}}
\toprule
\multirow{2}{*}{\textbf{Feature}} & \multicolumn{6}{c}{\textbf{Likelihood-Based}} & \multicolumn{2}{c}{\textbf{Likelihood-Free}} \\
\cmidrule(lr){2-7} \cmidrule(lr){8-9}
 & MLE & MCMC & PMCMC & SMC & INLA & VI & ABC & BSL \\
\midrule
\textbf{Theoretical Considerations} & & & & & & & & \\
\midrule
Requires likelihood? & Yes & Yes & Yes & Yes & Yes & Yes & No & No \\
Handles high-dim. params.? & Poor & Moderate & Moderate & Good & Good & Good & Moderate & Moderate \\
Convergence guarantees & Asymptotic & Asymptotic & Asymptotic & Asymptotic & Approx. & Approx. & Approx. & Approx. \\
Distributional flexibility & Low & High & High & High & Medium & Medium & High & Medium \\
Approximation error & Exact (asymp.) & Exact (asymp.) & Exact (asymp.) & Particle-based & Deterministic & Variational & Simulation & Simulation \\
\midrule
\textbf{Practical Considerations} & & & & & & & & \\
\midrule
Computational cost & Low & High & Very high & Med--High & Low & Low--Med & High & High \\
Scalability (big data) & Good & Poor & Poor & Moderate & Good & Good & Moderate & Moderate \\
Example software & --- & Stan, PyMC, JAGS, NIMBLE & . & LibBi & R-INLA & Stan, PyMC, Turing.jl & abc, abctools, EasyABC & --- \\
Tuning required? & Minimal & Step size, priors & Complex & Resampling & Minimal & ELBO opt. & Summary stats, distance metric, threshold & Summary stats \\
Capable of real-time inference? & Yes & No & No & Yes & Yes & Yes & No & No \\
Parallelization potential & 
High & 
Limited, GPU possible but hard & 
Limited, GPU possible for simple models & 
High, GPU possible & 
Low, hard to parallelise & 
Medium, gradient parallelisation; GPU possible & 
High, simulations parallelisable; GPU possible & 
Medium, GPU possible \\
Best use case & Simple models & General Bayesian inference & State-space models & Real-time inference & Latent Gaussian models & Fast approximation & Intractable likelihood & Intractable likelihood + summary stats \\
\bottomrule
\end{tabular}
\end{table}
\end{landscape}

\subsection{Outbreak Evolution and Workflow Iteration}

Two types of changes during outbreaks require revisiting this workflow.
First, new information emerges about pathogen characteristics, surveillance capabilities, and population dynamics that may necessitate updating model assumptions and data integration choices \citep{mccaw2023role}.
Second, research questions evolve as outbreak priorities shift from initial detection and characterisation towards intervention evaluation and long-term planning.
When these changes occur, practitioners should iterate through the workflow, visiting their earlier decisions, rather than developing entirely new models.

Changes that warrant workflow iteration include: substantial new data sources becoming available (such as genomic surveillance or serological surveys); shifts in research questions that alter target estimands; and improved understanding of pathogen biology that changes process model assumptions \citep{knock2021key}.
The modular approach we advocate enables practitioners to update specific workflow components whilst preserving elements from previous iterations.
For example, observation models developed for case surveillance can often be reused when adding death reporting, whilst process models for transmission dynamics may require updating when new variants emerge.
This incremental approach reduces development time and maintains continuity across evolving outbreak contexts.

These steps are often carried out on an ad-hoc basis by teams during outbreaks but we think that a more formal approach that tracks decisions is likely to improve the quality of models produced, and be ultimately less resource intensive.
Establishing regular review points to assess whether workflow iteration is needed is likely to be beneficial with more regular reviews during acute outbreak phases.
Each iteration should document what has changed, why updates are necessary, and how new approaches build upon previous work.
Having this documentation supports both real-time decision-making and retrospective evaluation of analytical approaches.

\subsection{Workflow Reporting}

Complex multi-source models require clear documentation to enable evaluation and reproduction.
Without structured reporting, even well-designed workflows are difficult to assess or improve upon.

Visual schematics should clearly communicate model architecture.
Process and observation DAGs should be reported separately, with integration methods explicitly diagrammed if multi-step methods are used.
For example, Ward et al.'s analysis of COVID-19 hospitalisation and fatality risk provides useful visual communication of model relationships but would benefit from process DAGs and code availability [@Ward2024-sp].

Validation procedures should be documented comprehensively: prior and posterior predictive checks, conflict detection, and sensitivity analyses.
Abbott et al.'s COVID-19 Delta variant analysis demonstrates reporting with code, data and posterior predictions, though lacks prior predictive checks and integration conflict discussion [@Abbott2021-delta].
Readers cannot fully assess model performance without diagnostic outputs, as illustrated by recent preprints where validation procedures are unreported [@Fyles2024-qz].

Code should be shared alongside results because it enables verification and accelerates progress.
For example, Abbott et al.'s prevalence estimation work has complete GitHub code, with clear links to model verification utilities, alongside the write up[@Abbott2022-prevalence].
Not sharing code, sharing it on request [@Ward2024-sp], or only once a paper is in a peer reviewed journal [@Fyles2024-qz] all create barriers to assess data integration workflows.

When data cannot be shared, synthetic data generation helps readers understand model behaviour.
For example, Mellor et al.'s norovirus nowcasting study provides training data with statistical noise added to preserve anonymity [@Mellor2025-norovirus].
Progressive disclosure is preferable to indefinite withholding.
Even incomplete sharing - mathematical specifications, validation code, synthetic examples - supports scientific progress.

Throughout our workflow, we emphasise making decisions transparent at each stage.
These decisions should be reported alongside the results from the work, including rationales for data source selection, integration method choices, model structure assumptions, and validation procedures undertaken.
This reporting enables both evaluation of current work and informed adaptation for future applications.

Ideally, version control should be used for workflow development with the full git history available to demonstrate how models evolved during development.
Even more ideally, to gain trust, the full analysis should be public as much as possible at all stages [@Abbott2021-delta; @Abbott2022-prevalence].
However, this may need careful management when working with confidential data sources.
Progressive disclosure of development history builds confidence in analytical approaches and enables learning from both successes and challenges encountered during model development.

\section{Case Studies}
% Lead: Anne Cori

\subsection{Overview}

We demonstrate our iterative workflow through four progressive case studies, each building complexity whilst showing different integration choices and methodological considerations.
All case studies share a common estimand -- the time-varying reproduction number ($R_t$) -- while the final case study has the overdispersion parameter ($k$) as an additional estimand. The case studies use SARS-CoV-2 outbreak data to leverage expert survey evidence on data source characteristics.
Each case study follows our complete workflow framework, demonstrating how adding more data sources enables analyses to answer questions that would otherwise require strong assumptions.

The progression between case studies illustrates key principles: case counts are often used to monitor transmissibiliy over time but require assumptions on the fraction of infections reported (CS0); data on deaths can eliminate reporting fraction assumptions that beset case-only analyses (if the case fatality ratio is known, CS1); wastewater data eliminates testing bias inherent in symptomatic surveillance (but is limited by uncertainty in the average shedding rate per infected individual, CS2); individual-level data enables direct overdispersion estimation without distributional assumptions (CS3).
Implementation choices reflect practical considerations including available expertise, computational resources, time availability, and model complexity trade-offs, whilst generally favouring efficient methods like NUTS where applicable.

TODO: need to add something on data source mapping which all the case studies will draw from: what data are available that may inform Rt and superspreading estimates? (Can draw from the survey and Anne C's slides at CIRM). What are their pluses and minuses (if we cannot do the full mapping from survey we can still discuss - otherwise refer to that section).

\subsection{Data survey}

\paragraph{}We refer back to the data sources and workflow frameworks introduced in Sections XXX and YYY. This section presents a case study illustrating the application of our proposed evaluation framework. 

\paragraph{}As the authors comprise individuals from diverse disciplinary backgrounds with experience working across a wide range of data types, we designed a questionnaire informed by a broad spectrum of perspectives. Building on the checklist of data characteristics proposed to assess the strengths and weaknesses of data streams, we developed targeted questions aligned with each characteristic to facilitate systematic and accessible characterisation. These questions (see Supplementary Material XXX) were designed to support both qualitative and quantitative evaluation across data sources. 

\paragraph{}Following the development of this questionnaire, both the authors and[??] completed it for a range of data sources. This exercise revealed a notable variation in how different individuals characterised the same data streams, reflecting differences in disciplinary perspective, objectives of the usage of data, xxx and yyy other reasons. We present these results in the Supplementary Material XXX. 

\paragraph{}Next, we agreed upon a focused set of data sources and their relevant characteristics considered in this case study. These data sources are a, b, c. We present a table (Table XXX) that summarises the potential strengths and weaknesses of these data. We noted that [what do we see in this table, will do after I write the table!]... 

\paragraph{}In addition to this view, we conducted detailed assessments of individual data sources to explore how these characteristics manifest[?] in practice and to identify the context-specific strengths and limitations. Detailed views of these are presented in Tables xxx [to do: and write what you see in them]

CALL BACK TO THE DATA SOURCES WORKFLOW PROPOSAL
SAY THIS IS A CASE STUDY OF DOING THIS

We made a big list across the authors using our approach. It was highly variable different views. useful to get it into text/data. See it in the SI

We then decided on a set of characteristics for the data sourves we consider in this case study. A focussed version of this is in table blah. As you can see blah blah blah
The full version of this is in SI. Interesting comment on free text

It is also useful to look in detail at individual data sources. Table blah is an example of this. Note blah and blah and blah.
We have the rest of thesee in the SI. 
The following case studies will demonstrate referring back to these and blah blah.



\subsection{Case Study 0: Single-Source Baseline (Cases Only)}


The research question for this case study (and case studies 1 and 2) is: what is the time-varying reproduction number $R_t$ during this outbreak period?
The objective of this case study is to showcase the workflow and explore some of the assumptions and modelling and fitting choices underlying estimation of $R_t$ from a single time series of case incidence.



%\textbf{Workflow Demonstration:}
%\begin{enumerate}
%    \item \textbf{Process DAG:} Simple renewal equation linking Rt to case incidence through generation time distribution, incorporating overdispersion via negative binomial model
%    \item \textbf{Data Source Mapping:} Case incidence time series selected based on expert survey ratings showing high timeliness but moderate precision due to testing policy effects
%    \item \textbf{Observation DAG:} Reporting delays, underascertainment fraction, weekend reporting effects
%    \item \textbf{Integration Choice:} Single source (no integration required)
%    \item \textbf{Implementation:} NUTS sampling of renewal equation with negative binomial observation model, chosen for computational efficiency and robust performance
%\end{enumerate}

%\textbf{Key Limitation:} Establishes baseline uncertainty but requires strong assumptions about generation time distribution and reporting fraction, highlighting assumption-dependent limitations that motivate multi-source approaches.

\textbf{Workflow Demonstration:}
\begin{enumerate}
    \item \textbf{Process model}: We consider three variants of the renewal process model for the number of new infections $I_t$ on day $t$, which is influenced by the reproduction number $R_t$ on day $t$, and the generation time (GT) distribution, which we assume to be fixed. The three model variants are:
\begin{itemize}
    \item[P1.] No variance in daily infection incidence (deterministic)
    \begin{equation} \label{eq:infections_P1}
        I_t = R_t \sum_{s=1}^\infty I_{t-s}w_s 
    \end{equation}
    \item[P2.] Poisson-distributed daily infection incidence.
        \begin{equation} \label{eq:infections_P2}
        I_t \sim \mathrm{Poiss}\left( R_t \sum_{s=1}^\infty I_{t-s}w_s  \right)
    \end{equation}
    \item[P3.] Negative binomially distributed daily infection incidence. 
            \begin{equation} \label{eq:infections_P3}
        I_t \sim \mathrm{NegBin}\left( R_t \sum_{s=1}^\infty I_{t-s}w_s, k  \right)
    \end{equation}
\end{itemize}
where $w_s$ is the probability mass function for the GT distribution and $k$ is the dispersion parameter. 
The process DAGs for these three models are shown in Figure \ref{fig:CS0_DAGs}.

 
% **********MJP: I think we (SA+AC+MP) agreed this is now covered already in the "Data sources and characteristics" section. But having read ahead to CS1 and 2, maybe it is useful to provide a brief remark here to illustrate the owrkflow? 
\item \textbf{Data source mapping}: Case incidence data is typically readily available with very frequent (often daily) updates and high timeliness. However, it is an imperfect and potentially biased proxy for infection incidence, and is dependent on testing patterns and lab processing and reporting timeliness. 


\item \textbf{Observation model}: The observation model relates the observed data (here daily case incidence) with underlying latent variables (here daily infection incidence). 
This requires assumptions to be made about underreporting (what fraction of infections are detected cases), reporting delays (time between infection and a case being detected and reported), and random noise (typically used to capture other things not explicitly specified in the model).

Alternative versions of the observation model can be considered, which either ignore or include each of these three aspects. This leads to $2^3=8$ possible observation models, each of which has an associated DAG (although some DAGs may be the same).
We refer to these models as $O_{000}$ (for the simplest observation model not modelling any of the above) to $O_{111}$ (for the most comprehensive accounting for all three observation features). 
%%%%%%%%%%%%% MJP: The following is an attempt to be more explicit about the equations for the observation model - we could delete this or move it to SI or to a Figure if we don't want this level of detail in the text. 
The $O_{111}$ model could be represented by the following equation for the observed variable $\hat{I}_t$, representing the number of reported cases on day $t$:
\begin{equation}
    \hat{I}_t \sim \mathrm{Poiss}\left( \sum_{s=0}^\infty N_{s,t}\right)
\end{equation}
where $N_{s,t}$ is the number of people infected on day $s$ and reported as a case on day $t$, given by
\begin{equation}
    N_{s,t} \sim \mathrm{TruncatedMultinomial}\left( I_s, \alpha_{t-s}\right) 
\end{equation}
and $\alpha_s$ is the proportion of infections that are reported as a case on the $s^\mathrm{th}$ day after infection ($s=0,1,\ldots$). 


\item \textbf{Fitting and integration approach}: As there is only one data source in this case study, no decisions about what and how to integrate multiple sources are needed. The choice of fitting method may depend on considerations such as analytical tractability of the combined process and observation model, computational complexity and speed of computation relative to the time available, and identifiability issues. 
 
% \begin{itemize}
%     \item Fitting choice discussion points to add:
%     \item Analytically tractable or not. Depends on the specific choices of PRIOR, P and O, the resulting model has an analytic formulation or not which guides choice here
%     \item Identifiability issues
%     \item Computational complexity / speed of computation
% \end{itemize}

One scenario where integration would be required is model ensembling, where multiple estimates of the target estimand (in this case $R_t$) are produced by different groups or methods based on the same data, and synthesised into a combined estimate. [***Link to SPI-M paper from Oxford]. In this context, it may be desirable to align the assumptions made by different models, for example about the GT distribution or the temporal smoothness in $R_t$, to ensure that estimates from the different models are  comparable [***Johannes’ paper].

\item \textbf{Implementation}: Implementation requires selection of one process model, one observation model and one fitting and integration method (which will depend on the choice of process and observation model as described above). For this case study, combinations of the 3 process models and 8 observation models give 24 possible models, on top of which alternative fitting methods can be chosen for each. Here, we give a few examples from the literature:
\begin{itemize}
    \item EpiEstim. This method assumes Poisson distributed infections, no underreporting, no delays in reporting, and no observation noise \citep{cori2013new}. In our categorisation, this corresponds to process model P2 and observation model $O_{000}$. The model is fitted by calculating an analytic posterior distribution for $R_t$ from a conjugate prior, which is fast and computationally efficient. 
    \item EpiNow2 [***may need to specify a version]. This method assumes deterministic infection incidence with no underreporting, but with delays and random observation noise. In our categorisation, this corresponds $P_1-O_{011}$. This is fitted using XXX.
    \item Epidemia or EpiMap for NB process – TODO: check what they do!
\end{itemize}

Other possible choices of fitting methods that could be considered / why not (refer to relevant section in the overall framework description)
 \end{enumerate}
 
% TODO: add a Figure. This will be A visual of jigsaw puzzle presenting a library of P (process models), O (observation models) and F (fitting methods). In multi-data ones we will need to add an I (integration method) too.
% Example choices of EpiEstim and EpiNow2 for example (as above). Prototype figure below

% \begin{figure}
% \includegraphics[width=0.75\textwidth]{figures/cs0_diagram1.jpg}
% \label{fig:CS0_DAGs}
% \caption{Process model DAGs for case study 0.}
% \end{figure}

% \includegraphics[width=0.75\textwidth]{figures/cs0_diagram2.jpg}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/Case study puzzle.png}
    \caption{A visual of jigsaw puzzle presenting a library of process models, observation models, integration methods, and fitting methods.}
    \label{fig:case_study_visual}
\end{figure}

\textbf{Question to Anne P} – can we write the model (e.g. assuming Poisson likelihood) as part of the DAG or is that something separate? If separate then we should make it clear throughout that the DAG isn’t the full model spec.

Different approaches have different strengths and limitations. For example, the key strength of EpiEstim is computational speed, whereas its key weaknesses are ignoring underreporting and reporting delays. The former may be less important in contexts where the reporting fraction can reasonably be assumed to be steady over the time period of interest. Furthermore, any change in reporting fraction will not be identifiable from a case time series alone, so it would not be sensible to attempt to estimate this in this example. EpiNow2 overcomes some of these weaknesses, at the cost of increased computational complexity. The more appropriate option may depend on the balance between need for timely estimates (e.g. for modelling an outbreak in real-time) and the realism of the associated assumptions. 

A common weakness of all the models outlined above is that they assume the GT distribution is constant and perfectly known. Some approaches (e.g. some versions of EpiEstim) extend the approach to relax this assumption (see also Case Study 4).
 
% \textbf{Next steps}:
% \begin{itemize}
%     \item feedback from others on case study 0
%     \item Anne C to draft similar case study 1 (cases + deaths)
%     \item Someone with skills to make nice picture of my horrific handwritten plots
% \end{itemize}




\subsection{Case Study 1: Two-Source Integration (Cases and Deaths)}

The target estimand for this case study is the same as in case study 0 ($R_t$). The objective of this case study is to illustrate the process and challenges of adding an additional data stream (in this case death data), and to explore how this can improve $R_t$ estimation and reduce assumption dependence.

\textbf{Workflow Demonstration:}
\begin{enumerate}
    \item \textbf{Process model iteration:} The number of deaths $D_t$ on day $t$ depends on the time series of infections up to day $t$, the infection-fatality ratio $p_d$, and the distribution $v_s$ of time ($s$) from infection to death. This can be represented in the process DAG (Figure X) and by the following convolution equation:
    \begin{equation} \label{eq:deaths}
        D_t \sim \mathrm{Poiss}\left(p_d \sum_{s=1}^\infty I_{t-s}v_s \right)
    \end{equation}
    Other process models for deaths are possible, for example using a multinomial model for the number of deaths occurring on day $s+k$ due to infections on day $s$ ($k=1,2,\ldots$). However, the Poisson model is a good approximation provided $p_d\ll 1$. This formulationassumes that the probability $p_d$ of an infection resulting in death and the distribution of time from infection to death $v_s$ are fixed and known.
    
    Eq. \eqref{eq:deaths} provides a model for the time series of daily deaths $D_t$ conditional on the time series of daily infections $I_t$ and would typically be used in combination with one of Eqs. \eqref{eq:infections_P1}--\eqref{eq:infections_P3} that describe the dynamics of infections to produce a joint model for infections and deaths. 
    \item \textbf{Data source mapping:} Expert survey shows deaths have higher reporting delays but lower noise (e.g. due to "day-of-the-week" reporting effects), higher ascertainment, and less policy dependence than cases (e.g. due to changing testing patterns).
    \item \textbf{Observation model:} Similar to the different choices of observation model for cases described in case study 0 (and still relevant to this case study), the observation model for deaths could include underreporting, reporting delays, and random noise. In jurisdictions with comprehensive death records, it may be reasonable to assume all deaths are reported and there is no additional observation noise, but it may still be important to account for observation delays, for example delay from date of occurrence to date of registration. 
    %%%%%%%%%%%%% MJP: The following is an attempt to be more explicit about the equations for the observation model - we could delete this or move it to SI or to a Figure if we don't want this level of detail in the text. 
    This would corresponds to observation model $O_{010}$, with the following equation for observed variable $\hat{D}_t$, representing the number of deaths registered on day $t$:
\begin{equation}
    \hat{D}_t = \sum_{s=0}^\infty M_{s,t}
\end{equation}
where $M_{s,t}$ is the number of deaths that occurred on day $s$ and were registered on day $t$, given by
\begin{equation}
    M_{s,t} \sim \mathrm{Multinomial}\left( D_s, \beta_{t-s}\right) 
\end{equation}
and $\beta_s$ is the probability mass function for the number of days from date of death to date of registration ($s=0,1,\ldots$).
    
    \item \textbf{Integration choice:} 
       Two different approaches are possible for integration: (1) a joint model including both cases and deaths with a shared latent state $R_t$; (2) separate models for cases and deaths, resulting in two estimates for $R_t$ to be combined via a weighted ensemble. 
       
       As outlined above, we recommend fitting separate models to the two time series initially, to understand their behaviour and reveal whether they lead to consistent or conflicting estimates of $R_t$. Where inconsistent results emerge, these could lead to refinement of the model (i.e. going back to step 1). For example, if $R_t$ estimates show similar trends but shifted in time, assumptions about delays may be revisited. If $R_t$ estimates are largely consistent, but the case-based estimate shows a transient increase in $R_t$ that does not occur in the deaths-based estimate, this could indicate a change in case ascertainment, which may require refining the model. Once this has been done, it may be desirable to combine the results into a single estimate by ensembling, or to fit a joint model that produces a single estimate from both data sources.  
       
    \item \textbf{Implementation:} For separate models, a good choice would be NUTS for estimation, followed by Markov melding to ensemble the results from the two models. For a joint model, a numerical method such as particle MCMC would likely be needed due to the state-space formulation complexity.

    \textcolor{red}{I think we should add some examples from the literature here. Just adding some notes for now:} 1) EpiNow2 seems to allow fitting jointly to cases and deaths: \url{https://wellcomeopenresearch.org/articles/5-112} - they use P1 for infections, but not very clear about cases and deaths - would need Sam's help here 2) this has several models based just on deaths - the last one uses cases and deaths but I don't think has a mechanisms for cases changing over time \url{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0286199} 3) I think Epidemic uses only deaths actually \url{https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-020-2405-7/MediaObjects/41586_2020_2405_MOESM1_ESM.pdf}

    \end{enumerate}

\textbf{Key Insight:} In case study 0 using data on cases alone, the case ascertainment rate (i.e. the proportion of infections that are reported as cases) was non-identifiable and had to be assumed. Including data on deaths, if the infection-fatality ratio is known, means that the case ascertainment rate could be included as a target for estimation. However, this requires assumptions about the distribution of time from infection to death, demonstrating how additional data sources can shift rather than eliminate the need for modelling assumptions.




\subsection{Case Study 2: Three-Source Integration (Cases, Deaths and Wastewater)}

The target estimand for this case study is the same as in the previous case studies ($R_t$). The objective of this case study is to explore ways of handling conflicting signals between data sources and incorporating complex observation processes.

\textbf{Workflow Demonstration:}
\begin{enumerate}
    \item \textbf{Process model iteration:} 
    Suppose that the average concentration $W_t$ of pathogen RNA/DNA in wastewater on day $t$ depends on the time series of infections up to day $t$ and the average amount of pathogen RNA/DNA that an individual sheds into wastewater $s$ days after being infected ($s=1,2,\ldots$). This can be represented in the process DAG (Figure X) and by the following equation for $W_t$:
    \begin{equation} \label{eq:wastewater}
        W_t \sim \Gamma\left( \frac{\alpha}{N_\mathrm{pop}}\sum_{s=1}^\infty I_{t-s}u_s, b   \right)
    \end{equation}
    where $\alpha>0$ is a constant representing the average total amount of pathogen RNA/DNA that an infected individual sheds over the course of their infection, $N_\mathrm{pop}$ is the population size, $u_s$ is the average shedding rate $s$ days after infection normalised such that $\sum_{s=1}^\infty u_s=1$, and $\Gamma(\mu,b)$ denotes a gamma distribution with mean $\mu$ and shape $b$. This was similar to the wastewater model used by REF; other choices of continuous non-negative-valued distributions are possible for $W_t$ such as a log-normal distribution.

    Eq. \eqref{eq:wastewater} would be combined with one of Eqs. \eqref{eq:infections_P1}--\eqref{eq:infections_P3} for infections and with Eq. \eqref{eq:deaths} for deaths.
    \item \textbf{Data Source Mapping:} Wastewater offers population-level signal independent of testing biases, with moderate timeliness but requiring environmental expertise. It could also be affected by unknown changes in the average shedding rate $\alpha$ over time. 
    \item \textbf{Observation model:} 
   Wastewater samples are typically collected at some cadence from one or more sampling sites, and the concentration of pathogen RNA in the samples is quantified via PCR testing. 
   The existence of multiple sites with different catchments populations and non-contemporaneous sampling frequencies complicates the interpretation of quantitative wastewater data, which can be modelled at varying levels of complexity.  
   
    Here, we assume that a noise-free measurement of the wastewater concentration $W_t$, either from a single representative site or a suitable average of multiple sites, is available on some subset of days $t$. Then the equation for the observed variable $\hat{W}_t$ is simply
    \begin{equation}
        \hat{W}_t=W_t
    \end{equation}
    If required, observation noise could be included by replacing this with a distribution with mean $W_t$. 
    More complex models could also incorporate other factors such as individual-level and site-level variation, catchment population dynamics, spatial heterogeneity, different sampling methods, and environmental degradation.
    \item \textbf{Integration choice:} As in case study 1, we recommend that a modular approach with sequential consistency assessment is initially taken to detect and resolve data source conflicts. For example, a time lag in estimates of $R_t$ from wastewater data alone relative to estimates of $R_t$ from cases alone could suggest misspecification of the shedding rate distribution $u_s$. Once conflicts have been identified and resolved, independent estimates of $R_t$ from these models could be combined by ensembling, or a joint model could be fitted with a shared $R_t$ state. 
    \item \textbf{Implementation:} ABC-SMC for wastewater sub-model due to likelihood intractability, reflecting available environmental modelling expertise; NUTS for case/death models. For a joint model, a method like particle MCMC would likely be needed due to the state-space complexity.
\end{enumerate}

\textbf{Key insight:} Wastewater data enables trend estimation independent of shifts in testing patterns, but requires assumptions about the shedding, environmental and sampling processes. A modular approach facilitates conflict detection between data sources. The average shedding rate $\alpha$ is typically unknown and non-identifiable from available data. Similar to the unknown case ascertainment rate in case study 0, estimates of $R_t$ will be insensitive to the value of $\alpha$ provided it does not change over time. However, changes in the average shedding rate due to, for example, pathogenic evolution or changes in population immunity, would invalidate this simple model and require a more nuanced approach.  



\subsection{Case Study 3: Individual-Level Data (Cases and Transmission Pairs)}

The research question for this case study is what is the time-varying reproduction number $R_t$ and how much heterogeneity is there in individual transmission rates? The latter is captured by the additional estimand $k$, representing the dispersion in the distribution of the number of secondary cases per index case. The objective of this case study is to illustrate how different types of data (time series count data and individual-level data from contact tracing records) can be combined to enable estimation of additional quantities. 


\textbf{Workflow Demonstration:}
\begin{enumerate}
    \item \textbf{Process model iteration:} Including information on individual-level variables, as opposed to just aggregate daily counts, requires a shift from the population-level renewal equation (Eqs. \eqref{eq:infections_P1}--\eqref{eq:infections_P3}) to an individual-level model which explicitly represents the transmission tree. A DAG for this type of model is shown in Figure \ref{fig:CS3_DAG}. Mathematically, this can be expressed in terms of the number $N_{i,t}$ of people newly infected by individual $i$ on day $t$:
    \begin{equation} \label{eq:individual_level}
    N_{i,t} \sim \mathrm{Poiss} \left( R_t w_{t-T_i} Y_i\right)
    \end{equation}
    where $T_i$ is the infection time of individual $i$ and $Y_i$ is a continuous, non-negative random variable with mean $1$, representing the relative transmission rate of individual $i$ (which may capture a combination of biological and social factors). This formulation provides a way to explicitly simulate the full epidemic transmission tree.  In the special case where $Var(Y)=0$ (i.e. there is no individual-level heterogeneity), the total number of new infections on day $t$ ($I_t$) reduces to Eq. \eqref{eq:infections_P2}. The model has the property that the total number of secondary cases $Z(t)$ arising from an individual infected on day $T$ and with given individual-level transmission parameter $Y$ is
    \begin{equation}
       Z(t) \ | \ Y \sim \mathrm{Poiss}\left( Y \tilde{R}_t\right)
    \end{equation}
    where $\tilde{R}_t= \sum_s R_{t+s} w_s$ is the effective reproduction number averaged over the infectious period of an individual who was infected on day $t$. If $Y$ has a gamma distribution, i.e. $Y_i\sim \Gamma(1,k)$ for some dispersion parameter $k$, then the number of secondary cases caused by a randomly selected individual infected on day $t$ is
     \begin{equation} \label{eq:offspring_dist}
        Z(t) \sim \mathrm{NegBin}\left( \tilde{R}_t, k\right)
    \end{equation}   

    Note that this model assumes that the relative infectiousness profile over time $w_s$ is fixed and identical for all individuals. This is a reasonable starting assumption, but it may be important to relax this in some situations, for example to model the impact of quarantine and isolation measures on individuals identified by contact tracing.
    \item \textbf{Data source mapping:} Case studies 0, 1 and 2 only consider aggregate-level data on daily counts or wastewater measurements. This type of data typically only allows estimation of average transmission, i.e. $R_t$. In contrast, contact tracing records identify transmission pairs, which contain information about heterogeneity in transmission patterns, contingent on contact tracing system quality. The smaller the dispersion parameter $k$, the more variance would be expected in the distribution of the number of secondary infections per index case.
    \item \textbf{Observation model:} We assume that, for some subset of reported cases, the number of observed secondary cases associated with that index case is recorded. For simplicity, we assume that the index cases for whom this data is available are a randomly chosen subset of all reported cases, and each secondary infection independently has fixed probability $p_l$ of being linked to the index case. We assume that all linked secondary cases are associated with the correct index case (i.e. we ignore any uncertainty in who infected whom). This implies that the number of recorded secondary cases for an index case who was infected on day $s$ follows a negative binomial distribution with mean $p_l\tilde{R}_s$ and dispersion parameter $k$. Conditioning on the index case's day of infection, the probability $P_t(n)$ that an index case who was reported on day $t$ has $n$ recorded secondary cases is 
    \begin{equation}
    P_t(n) = \sum_s \alpha_{t-s} F_{NB}\left(n \ ; \ p_l\hat{R}_s, k\right) 
    \end{equation}
    where $F_{NB}(.\ ;\ \mu, k)$ is the probability mass function for a negative binomial distribution with mean $\mu$ and dispersion $k$.
    This observation model for transmission pairs would be coupled with one of the observation model for reported cases described in case study 0.
    
 %   Eq. \eqref{eq:offspring_dist} can be used to derive the likelihood of a person who was infected on day $t$ causing a total of $n$ secondary infections, for a given value of the parameter $k$ and the state variables $R_{t+s}$ ($s=1,2,\ldots$). 
    \item \textbf{Integration choice:} A hierarchical model linking individual transmission events to population-level reproduction number would be a suitable framework.
    \item \textbf{Implementation:} NUTS with data augmentation for unobserved transmissions, chosen for efficient handling of discrete latent variables. Alternative a method such as particle marginal Metropolis Hastings could be used to used to estimate the time-varying reproduction number $R_t$ and the fixed parameter $k$ in a hierarchical manner (but may be computationally slow). 
\end{enumerate}

\textbf{Key Insight:} Individual-level data enables direct overdispersion estimation without distributional assumptions but requires contact tracing completeness assumptions. This shows how data granularity can fundamentally change model structure and inference requirements.

\begin{figure}
\centering
(a) \\
\includegraphics[width=0.75\textwidth]{figures/case_study_0_1_2.jpg}\\
(b)\\
\includegraphics[width=0.75\textwidth]{figures/case_study_3.jpg}
\label{fig:CS3_DAG}
\caption{[Mike's attempt at DAGs for the case studies (can be made prettier later on).] Latent states in blue, observed states in green. (a) Case studies 0--2. Process model P1 is a special case where $I_t$ is a deterministic funcvtion of $R_t$ and $I_{1:t-1}$; P2 is a special case in the limit $k\to\infty$; P3 allowing the dispersion parameter $k$ to take any given value. The different observation models described in case study 0 (with or without underreporting, reporting delay, and observation noise) would affect how the expected value of the observed state $C_t$ relates to the latent states $I_{1:t}$, and the variance of the observed state $C_t$. Case studies 1 and 2 add additional observed states for deaths ($D_t$) and wasterwater observations ($W_t$) respectively. (b) Case study 3 requires more states to be inlcuded in the DAG to take account of individual heterogeneity and the impact it has on obsetved data on transmission pairs. The variable $N_{it}$ representing the number of secondary infections caused by individual $i$ on day $t$ encodes the full transmission tree. The expected value of $N_{it}$ is determined by the instantaneous reproduction number $R_t$, the relative transmission rates $Y_i$ of previously infected individuals, and their current infectiousness determined by the generation time distribution (GT). The variance in the distrubiton of $Y_i$ is determined by the dispersion parameter $k$.  }
\end{figure}






\section{Discussion}

\subsection{Summary}

We have presented a framework for integrating multiple data sources in infectious disease modelling that extends established Bayesian workflow principles.
This workflow serves both as guidance for practitioners developing multi-source models and as a checklist for readers evaluating modelling papers.
Our Bayesian workflow comprises linked steps that begin with data source characterisation using a structured checklist evaluating metadata, scope, resolution, data quality, data utility, and practical considerations.
Building from this foundation, we suggest defining research questions and target estimands that shape all downstream decisions.
We then recommend representing epidemiological mechanisms through process DAGs, with structure evolving as understanding improves during outbreaks.
Next, data source mapping connects surveillance streams to process components based on their expected contributions to target estimands.
Observation DAGs follow, linking latent processes to measured data through reporting mechanisms, delays, and biases.
Integration choices must then address what to combine and how, balancing theoretical benefits against practical constraints.
Fitting method selection completes the core workflow, balancing statistical rigor with computational feasibility across likelihood-based and likelihood-free approaches.
Throughout, feedback loops connect these stages because downstream constraints often require revisiting earlier assumptions.
During outbreaks and other evolving settings, analysts should regularly review their workflow choices as new data becomes available and adapt their methods when expecting rapid change or repeated analyses with similar model requirements.

Four case studies demonstrate this workflow using progressively more data sources for reproduction number estimation.
Each progression shows how additional data sources alter both process and observation DAGs whilst requiring new integration and fitting decisions.
We establish the baseline approach using case data to estimate $R_t$ whilst highlighting assumptions about generation time and reporting fractions.
Adding death data eliminates some reporting assumptions but shifts dependence to infection-to-death delay assumptions and requires new integration choices.
Wastewater data provides population-level signals independent of testing biases but requires understanding viral shedding processes and likelihood-free fitting methods.
Individual-level contact tracing enables direct overdispersion estimation but leads to hierarchical process models and latent variable approaches which can be computationally challenging.
Each case study also leads to new fitting choices and model validation strategies.
Throughout we highlight that considering each submodel in isolation is a useful starting point for model development and validation.
Each case study illustrates real-world trade-offs between model complexity, computational feasibility, and parameter identifiability.

\subsection{Strengths and Limitations/Outstanding Challenges}

A key strength of our suggested workflow is the explicit separation of epidemiological process DAGs from observation DAGs, recognising that these components draw from different domains: epidemiological processes are informed by infectious disease theory whilst observation models reflect data source characteristics and collection mechanisms.
This separation provides clarity for model development and validation.
The framework's applicability across diverse contexts, from early outbreak analysis to routine surveillance, offers flexibility for varied epidemiological questions.
In addition, our suggested data review process provides a structured approach to understanding different epidemiological data sources.
By providing explicit guidance on reporting standards, including validation procedures and code availability, we aim to improve both the development and evaluation of multi-source models.

Our suggested approach can be implemented with current tools, though doing so may require choosing between statistical rigour (resource-intensive joint models) and computational feasibility (pipeline approaches with known limitations).
The DAG-based workflow helps analysts plan for evolving data availability, but adapting models as surveillance systems change typically requires substantial rework.
Teams may need to maintain multiple parallel models for different data scenarios, potentially limiting rapid response capabilities.

Without shared infrastructure, teams typically rebuild common epidemiological processes for each application.
Current probabilistic programming frameworks provide foundations but often lack the epidemiological abstractions needed for routine use.
These implementation barriers can lead teams to adopt simpler approaches despite understanding their statistical limitations.

Workflow reporting remains inconsistent across the field, with varying levels of detail about model validation, code availability, and decision rationales.
Even when models are technically sound, incomplete reporting limits their utility for learning and adaptation.
Establishing community standards for workflow documentation would accelerate progress by enabling systematic learning from both successes and failures.

\subsection{Comparison with Existing Literature}
% TODO: Position relative to pipeline vs joint modelling literature
% TODO: Relationship to evidence synthesis and meta-analysis methods
% TODO: Comparison with ensemble forecasting approaches
% TODO: Distinguish from existing reviews and methodological papers
% TODO: Compare with other multi-source integration frameworks

\subsection{Future work}

Future progress requires infrastructure that bridges the gap between methodological best practice and routine implementation.
Composable modelling tools, where individual self contained components can be combined to build more complex models, could enable modular development where domain experts contribute specialised components without rebuilding core functionality.
Such tools would need standardised interfaces between model components, epidemiological abstractions for common processes, and support for seamless reconfiguration as data availability evolves.

Key priorities include developing community standards for model specification and validation across multiple data sources.
Computational scalability remains critical, particularly for joint analysis of individual-level genomic data with population surveillance.
As emerging data sources from digital health systems and environmental monitoring become available, frameworks must incorporate these streams whilst maintaining interpretability.
Model validation procedures that can detect conflicts between data sources and assess component compatibility need development beyond current approaches.
These advances would make rigorous multi-source integration accessible to public health teams without prohibitive resource requirements, enabling the rapid response capabilities that outbreak situations demand.


\section{Conclusions}

Infectious disease modelling workflows benefit from systematic approaches that can adapt to multiple data sources and evolving outbreak contexts.
We have presented workflow best practices that provide practical guidance for navigating trade-offs between information gain, computational complexity, and interpretability across diverse modelling scenarios.
Our workflow extends established Bayesian principles through structured steps from data characterisation to model validation, supported by expert consensus on data source characteristics and progressive case studies.
The modular approach we advocate enables practitioners to build complexity incrementally whilst maintaining transparency about modelling assumptions and integration choices.
We recommend that the infectious disease modelling community adopt these workflow best practices for both model development and evaluation.
Readers can use our framework as a systematic checklist when evaluating modelling papers to assess whether workflow choices are appropriate and transparent.
Practitioners should implement these approaches across all infectious disease modelling contexts, with particular value during rapidly evolving outbreaks where research questions shift and new data sources emerge.
Establishing community standards around systematic workflow practices will improve both methodological rigour and practical implementation across the field.

\section{Acknowledgements}

We thank the organisers and participants of the workshop "Analysis and modelling for the design of future epidemic surveillance systems" (CIRM, Marseille, 28 April–2 May 2025) for valuable discussions and feedback that helped shape this work.
We are particularly grateful to Anne Cori, Sebastian Funk, James McCaw, and Freya Shearer for organising this productive workshop.
All workshop participants provided useful discussion and feedback.
We thank Poppy the dog for making sure to ask the important questions.

\bibliographystyle{plainnat}
\bibliography{references,zotero-references}


\end{document}
 